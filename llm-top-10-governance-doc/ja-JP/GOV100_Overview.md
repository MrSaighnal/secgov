## 第一章　概 要

すべてのインターネットユーザーと企業は、来るべき強力な生成的人工知能（生成AI）アプリケーションの波に備えなければなりません。生成AIは様々な業界に革新と効率化、商業的成功をもたらす大きな可能性を秘めている一方、発展の初期段階にある他の強力なテクノロジーと同様、予期せぬ新たな課題ももたらすからです。

人工知能は過去50年の間に著しい発展を遂げ、企業の様々なプロセスをサポートしてきましたが、多くは目立たないものでした。ChatGPTが広く知られるようになると、個人のレベル、並びに企業の間で、大規模言語モデル(Large Language Models, LLM)の開発と利用が推し進められることとなりました。当初、これらテクノロジーの用途は、学術的な研究や特定の企業の使用に限られており、一部の人にしか知られていませんでした。しかし、多くのデータが使えるようになり、コンピュータの能力が向上し、生成AIが進歩し、そしてLlama 2、ElevenLabs、Midjourneyのようなツールがリリースされるにおよび、AIはニッチなものから一般に広く受け入れられるものへと変貌をとげています。生成AI技術がより身近なものになっただけでなく、企業が業務にAIを統合し活用するためには、確たる戦略を策定する必要性が浮き彫りになってきたのです。

最初にいくつかの重要な用語を定義しておきましょう。
- 人工知能(AI)は、従来、人間の知能を必要としてきたタスクを、機械が達成できるようにするために必要な、コンピュータサイエンスの多くの分野を包含する用語です。機械学習と生成AIは、人工知能(AI)に含まれる2つの分野です。
- 機械学習はデータから学習できるアルゴリズムの作成に重点を置いています。機械学習アルゴリズムは、一連のデータを繰り返し入力することにより学習し、未知のデータの変動予測や分類を行うことができます。
- 生成AIは、機械学習アルゴリズムが新たなデータを作り出すことができるようにしたものです。
- 大規模言語モデル（LLM）は、人間と同じように文章を理解し、かつ、文章を作り出すことができる生成AIモデルの一種です。
- AIの用語で「モデル」とは、入力データに基づいて予測を行うように訓練されたシステムを指します。LLMは特に自然言語の大規模なデータ、すなわち大量の文章から学習されるため、大規模言語モデルと呼ばれています。

我々は今、生成AIソリューションの安全性の確保と管理という未知の領域に入ってきています。生成AIは、敵対者が攻撃戦略を高度化することにも使えるため、その急速な進歩は両刃の剣で、攻撃の脅威と防御の必要性が急速に拡大することも意味するのです。

この「LLM AI サイバーセキュリティとガバナンスのチェックリスト」は、コンテンツを自動生成するLLMを使ったアプリケーションに対する攻撃の脅威と防御に焦点をあて、解説していきます。


### 責任ある信頼できるAI

AIの利用をめぐる利点と課題が明らかになり、規制や法律が成立するにつれ、『責任ある信頼できる AI』の原則は、漠然とした懸念から、確立された基準へと進化しています。
[OWASP AI Exchange ワーキンググループ](https://owasp-ai-exchange.web.app/)は、このような変化を監視しAIのあらゆる側面における、広範で複雑な要件の検討に取り組んでいます。そのなかで、信頼できるAIの条件を次のように考えています。

>||center|16|16 信頼できる AI の条件

    >indianred|white|left|14|18 頼りになること（reliable）
    >indianred|white|left|12|16 　　　　頑丈にできていること（robust）
    >indianred|white|left|12|16 　　　　責任の所在が明らかなこと（accountable）
    >indianred|white|left|12|16 　　　　常に観察し記録していること（monitored）
    >indianred|white|left|12|16 　　　　透明性があること（transparent）
    >indianred|white|left|12|16 　　　　なぜかを説明できること（explainable）

    >forestgreen|white|left|14|18 強靭であること（resilience）
    >forestgreen|white|left|12|16 　　　　安全であること（safe）
    >forestgreen|white|left|12|16 　　　　安心できること（secure）
    >forestgreen|white|left|12|16 　　　　個人情報が守られていること（private）
    >forestgreen|white|left|12|16 　　　　効果的であること（effective）

    >dodgerblue|white|left|14|18 道に外れていないこと（responsible）
    >dodgerblue|white|left|12|16 　　　　公正であること（fair）
    >dodgerblue|white|left|12|16 　　　　道徳的に正しいこと（ethical）
    >dodgerblue|white|left|12|16 　　　　一部の人をのけ者にしないこと（inclusive）
    >dodgerblue|white|left|12|16 　　　　持続可能なこと（sustainable）
    >dodgerblue|white|left|12|16 　　　　はっきりした目的があること（purposeful）

##### 図 1.1　信頼できる AI の条件
##### 　　Montreal Ethics Institute Example より抜粋

### 対象読者

本書は、経営幹部、技術者、サイバーセキュリティ、プライバシー、コンプライアンス、法務、DevSecOps、MLSecOps、サイバーセキュリティチーム、および保守担当者の各分野のリーダーを対象としています。
AIを企業の成功のために活用するだけでなく、安全でないAIの実装や、AIの実装を急ぐことによるリスクにも注目し、急速に変化するAIの世界で、常に一歩先を行くための努力をしていらっしゃるリーダーやチームの皆さん、チャンスをつかみ、課題と戦い、リスクを軽減するための戦術を構築しようとされている全ての方々に読んでいただけることを狙いとしています。また、技術およびビジネス・リーダーの方々には、LLMを使用するリスクとメリットを理解し大規模言語モデル戦略を策定する際に、組織を防衛するために重要な領域とタスクの包括的なリストをいち早く作成する助けになることを目的としています。

OWASP 大規模言語モデル アプリケーション リスク トップ10 チームは、このリストが読者の皆様の組織で既にお持ちの防御手法の改善や、この新しいLLMテクノロジーを使用する際の脅威への対応策の開発に役立てることを願っています。

### なぜチェックリストが必要なのか

チェックリストは、戦略を策定する際に、正確性を向上させ、目的を明確にし、統一性を保ち、集中した慎重な作業を促し、見逃しや細部の見落としを減らします。チェックリストは、戦略策定への確度を高めるだけでなく、継続的な改善のためのシンプルで効果的な手段を提供し、将来の組織の革新をより確かなものにします。

### 将来にわたって網羅的ではない

本書は、急速に変化する技術的、法的、及び様々な規制の中で、組織が初期のLLM戦略を策定する際の支援を目的していますが、全ての使用ケースや方策を網羅するものではありません。時には、このチェックリストのもとにして、その使用ケースや管轄区域の必要に応じて、評価及び実務を拡張する必要がでてくるでしょう。

### 大規模言語モデルの課題

大規模な言語モデルは、いくつかの深刻で固有の問題に直面しています。最も重要なことの1つは、LLMの開発は、大量の訓練データを用いてモデルを作り出すので、「制御の流れ」と「データ」を厳密に区別、または分離することができないということです。通常のコンピュータ・プログラムでは「制御の流れ」の記述しているプログラムを修正することで、ソフトウエアの不具合を修正することができますが、LLMの不具合が特定できたとしても、訓練データのどこを変えると不具合が修正できるかは、多くの場合、一筋縄ではいかないのです。
もう1つの重大な課題は、LLMは設計上、非決定論的であり、プロンプト、リクエストが同じようでも、時々に異なる結果をもたらすことです。すなわち、LLMは意味論的な検索を採用しているので、モデルのアルゴリズムがレスポンスに含まれる語に微妙に異なった優先順位をつけることです。従来のインターネット検索で使われているキーワード検索とは大きく異なり、結果の一貫性と信頼性に影響を与えます。LLMでは「幻覚」と呼ばれる現象がああります。それは、モデルのトレーニングに使ったデータの欠落やデータの欠陥が引き起こす不確かさがある場合、生成プロセスが非決定論的な動作であるため、同じような状況でも結果が異なることにより生じます。

信頼性を向上させ、ジェイルブレイク(註1)、モデルトリック(註2)、幻覚に対する攻撃を防御する方法はありますが、攻撃手法が巧妙になってくると、防御策を実装するコストとその実効性の間でトレードオフをする必要が出てきます。

【註】
1. ジェイルブレイク
  従来のコンピュータでは、制限されている機能にアクセスする方法を見つけ出し、実行することを「ジェイルブレイク」という。LLMアプリケーションでは、ユーザーがプロンプトを巧妙に仕組むことによりLLMアプリに有害な動作をするように仕向けること。LLMが進歩するにつれ、ジェイルブレイクにたいする防御も進歩しているが、攻撃者によるアタック手法も巧妙になっている。
2. モデルトリック
  モデルの訓練に使用されたデータの不完全性をついて、「全く同じように見えるデータだが、モデルは全く異なる結果をだす」場合や「全く異なるように見えるデータだが同じ結果をだす」という欠陥を見つけ出し、誤動作にいたらしめること。

LLMの使用が増加するにつれ、従来のソフトウエアには無かった領域が攻撃対象となり、リスクが増大します。LLMアプリへの攻撃には、そのような未知のものもありまずが、既知の攻撃手法を使った場合が多いのも事実です。たとえば、ソフトウェア部品表（SBoM）、サプライチェーン、データ損失保護（DLP）、認証されたアクセスなど、多くはおなじみの問題です。また、生成AIの進歩は、攻撃者が生成AIを利用すれば、攻撃者の効率性、能力、有効性を高めることになります。

【註】
3. ソフトウェア部品表（SBoM）
  自らのソフトウエア中で使用するオープンソース・ソフトウエアなど第三者が作成したもの
4. サプライチェーン
  自らのソフトウエア中で使用するベンダーが作成したもの
5. データ損失保護（DLP）
  重要データが使用中、転送中、保管中に失われたり、漏えいすることから守ること
6. 認証されたアクセス
  ソフトウエア・ランセンスを受けることにより受領者に与えられるアクセスならびに使用権

攻撃者は、LLMと生成AIを利用し、組織、個人、政府システムを攻撃する手法を、いち早く変えてきています。LLMは、新しいゼロデイ脆弱性（註7）を埋め込んだり、検知を回避するように設計された新しいマルウェア（註8）の作成を、容易にするために使うことができるのです。また、LLMを使って今までにない、より巧妙なフィッシング詐欺（註9）の手口を生み出すことも可能です。動画や音声を偽造するなどして、ソーシャルエンジニアリング（註10）の攻撃をさらに高度な、見分けにくいものにできます。さらに、AIツールは、侵入を実行し、巧妙なハッキング手法を開発することにも使えます。今後、AI技術を利用して、個々の攻撃対象に特化した犯罪、かつ複合的な犯罪が発生することも十分に考えられます。それに対し、適切な防御や攻撃されてしまった場合の回復に対する特別な解決策が求められるようになってきます。

【註】
7. ゼロデイ脆弱性
  開発チームに紛れ込んだハッカーによりソフトウエアのリリース前に埋め込まれた脆弱性
8. マルウェア
  コンピューターやその利用者に被害をもたらすことを目的とした、悪意のあるソフトウェアのこと。ウイルスは、プログラムの一部を書き換え、自己増殖していくマルウェアです。
9. フィッシング詐欺
  送信者を詐称した電子メールを送りつけたり、偽の電子メールから偽のホームページに接続させたりするなどの方法で、クレジットカード番号、アカウント情報（ユーザID、パスワードなど）といった重要な情報を盗み出す行為
  [総務省　国民のためのサイバーセキュリティサイト](https://www.soumu.go.jp/main_sosiki/cybersecurity/kokumin/enduser/enduser_security01_05.html)より
10. ソーシャルエンジニアリング
  ネットワークに侵入するために必要となるパスワードなどの重要な情報を、情報通信技術を使用せずに盗み出す方法です。 その多くは人間の心理的な隙や行動のミスにつけ込むものです。

組織はまた、LLMを活用しない場合に発生する脅威にも直面しています。例えば、競争上の不利、顧客やパートナーからの時代遅れという市場認識、パーソナライズされたコミュニケーションの拡張不能、イノベーションの停滞、運用上の不備、プロセスにおける人為的ミスのリスクの増大、人的資源の不適切な配分などです。

さまざまな種類の脅威を理解しビジネス戦略と統合すれば、大規模言語モデル（LLM）を使用する場合、使用しない場合のメリットとデメリットを比較検討して、そのデメリットばかりを恐れることによりビジネス目標の達成を妨げるのではなく、むしろ加速させるように、LLMの導入戦略を見極めることができるのです。

### AIの脅威の分類

>||center|16|16 AIの脅威

    >cornflower|white|left|14|18 AI モデルを使わないことによる脅威
    >cornflower|white|left|14|18 AI モデルを使うことによる脅威
    >dodgerblue|white|left|14|18 AI モデルが狙われることによる脅威
    >fidblue|white|left|14|18 AI モデルが生み出す脅威
    >darkblue|white|left|14|18 AI 法律と規制による脅威

##### 図 1.2　AIの脅威の分類
##### 　　作成 sdunn



### AIのセキュリティとプライバシーのトレーニング

AIと生成AIを理解すること、並びにLLMの構築・購入・利用により将来おこり得る事象を理解するためのトレーニングは、組織全体の従業員にとって有益です。LLMの、許される使い方と許されない使い方、セキュリティ意識に関するトレーニングは、すべての従業員を対象とするものだけでなく、人事部、法務部、開発者、データチーム、セキュリティチームなど、特定の職種に特化したものも必要です。

AIを公正に使用するための基礎と健全なAIとは何かを、早い段階から理解することは、将来のAIサイバーセキュリティ啓発の成功の礎となります。ユーザーはAIに対する基本的な対応の仕方だけでなく、AIが社会にもたらすの功罪、倫理的かどうかを判断する規範を身につけることができるのです。

### LLMのセキュリティとガバナンスを既存の施策に組み込む

AIは、サイバーセキュリティ、プライバシー、法律や様々な規制への対応に新たな局面をもたらしますが、問題を特定し、脆弱性を発見し、脆弱性を修正し、潜在的なセキュリティ問題を軽減する最善の方法は以下のように目新しいものではありません。

- AIシステムの管理方法が、既存の組織慣行と統合されていることを確認する。
- AIシステムが、既存のプライバシー・ガバナンス・セキュリティ施策に従うことを確認する。
- 必要に応じて、AIに固有のプライバシー・ガバナンス・セキュリティ施策を策定する。

### セキュリティの基本原則

LLMの機能は、異なるタイプの攻撃と攻撃対象領域をもたらします。LLMは、プロンプト・インジェクション、安全でないプラグイン設計、リモート・コード実行などの複雑なビジネス・ロジックのバグに対して脆弱です。そのようなビジネス・ロジックのバグを解決するには、既存の施策が役立ちます。社内の製品セキュリティチームが、セキュアなソフトウェアのレビュー、アーキテクチャ、データガバナンス、サードパーティの評価を理解しているかどうか。サイバーセキュリティチームは、更に、音声クローニング（註11）、なりすまし（註12）、キャプチャ（註13）のバイパスなど、LLMによって影響が大きくなる問題を見つけるために、十分な対応をしているかチェックする必要があります。
機械学習、NLP（自然言語処理）、NLU（自然言語理解）、ディープラーニング、そしてLLM（大規模言語モデル）や生成AIの最近の進歩を考慮すると、サイバーセキュリティ・チームやソフトウエア開発管理チームと一緒にこれらの技術分野に精通した専門家を含めることが重要になってきます。彼らの専門知識は、組織がこれらの技術を採用するのに必要なだけでなく、新たな課題に対する革新的な分析や対応を開発する上でも役立ちます。

【註】
11. 音声クローニング
  個人の録音された音声を分析し、その音声の高低、特徴などを使って、会話を合成すること
12. なりすまし（impersonation）
  音声だけでなく、行動や、容姿などをビデオ、画像を合成すること
13. キャプチャ
  オンライン・ログインの際に、パスワード入力の他に、人間であること確認するために作った仕組みで、画像の種類を判断させるなどの手法がある

### リスク

ISO31000（註）の定義では、リスクとは「目的に対する不確実性の影響」です。本チェックリストで述べるLLMリスクは、敵対的リスク、安全リスク、 法律リスク、規制リスク、世評リスク、財務リスク、競争リスクに対応しています。

【註】
  [ISO31000](https://www.assp.org/standards/standards-topics/risk-management-iso-31000#:~:text=The%20ISO%2031000%20standards%20provide,and%20explain%20risk%20management%20techniques.)

### 脆弱性と対応策

脆弱性を分類し脅威情報を共有するために開発された OVAL（註14）、STIX（註15）、CVE（註16）、CWE（註17）などのシステムは、大規模言語モデル（LLM）や予測モデルに固有の脆弱性や脅威を監視し、攻撃があった場合、警告する機能をまだ開発中です。AI/MLシステムとそのサプライチェーンに対する脆弱性や脅威が見つかった際には、脆弱性分析のためのCVEやサイバー脅威インテリジェンス（CTI）の情報交換のためのSTIXのような、確立し広く使われている標準は大きな助けになります。

【註】
14. OVAL
  [Open Vulnerability and Assessment Language](https://oval.mitre.org/)
15. STIX
  [Structured Threat Information eXpression](https://stixproject.github.io/)
16. CVE
  [Common Vulnerabilities and Exposures](https://cve.mitre.org/)
17. CWE
  [Common Weakness Enumeration](https://cwe.mitre.org/)
