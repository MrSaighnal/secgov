## 第三章　チェックリスト

### 敵対的リスク

敵対的リスクには、競合他社や攻撃者が含まれます。

- 競合他社がどのように人工知能に投資しているかを精査してください。AIの導入にはリスクがありますが、将来の市場ポジションに影響を与えるビジネス上のメリットもあります
- 音声認識を利用したパスワードのリセットなど、新たな生成AIによる攻撃から身を守るための適切なセキュリティがもはや提供されていない可能性のある現行の管理策の影響を調査します。
- インシデント・レスポンス・プランとプレイブックを更新し、生成AI拡張攻撃とAIML 特有のインシデントに対応。

### 脅威のモデリング

脅威を特定し、プロセスとセキュリティ防御を検討するために、脅威モデリングを強く推奨します。脅威モデリングは、アプリケーション、ソフトウェア、システムに対して合理的なセキュリティ決定を行うことを可能にする、体系的で反復可能な一連のプロセスです。生成AIが加速する攻撃やLLMを導入する前の脅威モデリングは、リスクを特定し、軽減し、データを保護し、プライバシーを保護し、ビジネス内の安全でコンプライアンスに準拠した統合を確実にするための最も費用対効果の高い方法です。

- 攻撃者は、組織、従業員、経営陣、ユーザーに対する攻撃をどのように加速させるのでしょうか？組織は、Generative AIを使用した大規模な「ハイパーパーソナライズド」攻撃を予測する必要があります。LLMが支援するスピア・フィッシング攻撃は、今や指数関数的に効果的で、標的が絞られ、攻撃の武器となります。
- 生成AIは、スプーフィングや生成AIが生成したコンテンツを通じて、企業の顧客やクライアントへの攻撃にどのように利用される可能性がありますか？
- LLMソリューションへの有害または悪意のある入力やクエリを検出し、無効化できますか？
- LLMのすべての信頼境界において、安全な統合により既存のシステムやデータベースとの接続を保護できますか？
- 許可されたユーザーによる悪用を防ぐために、内部脅威の緩和策を講じていますか？
- 知的財産を保護するために、独自のモデルやデータへの不正アクセスを防止できますか？
- コンテンツフィルタリングを自動化することで、有害または不適切なコンテンツの生成を防ぐことができますか？

### AI資産目録

AI資産目録は、社内で開発されたソリューションと外部またはサードパーティのソリューションの両方に適用する必要があります。

- 既存のAIサービス、ツール、所有者をカタログ化します。資産管理で特定のインベントリのタグを指定します。
- ソフトウェア部品表（SBOM）は、アプリケーションに関連するすべてのソフトウェア部品、依存関係、およびメタデータの包括的なリストです。
- AIデータソースのカタログとデータの機密性（保護、機密、公開）
- 現在の攻撃サーフェスのリスクを判断するために、配備されたAIソリューションのペンテストまたはレッドチームが必要かどうかを確認します。
- AIソリューションのオンボーディング・プロセスの構築
- SBoM の要件に従い、熟練したIT管理スタッフを社内または社外から確保します。

### AIセキュリティ・プライバシー研修

- 従業員と積極的に関わり、LLMの計画的な取り組みについて理解し、懸念に対処します。
- 組織のプロセス、システム、従業員の管理とサポート、顧客との関係における予測的または生成的なAIの使用と、その使用がどのように管理され、リスク対処されるかについて、オープンで透明性のあるコミュニケーション文化を確立すること。
- 倫理、責任、および保証、ライセンス、著作権などの法的問題について、すべてのユーザーを訓練します。
- 生成AI関連の脅威を含むセキュリティ意識向上トレーニングの更新。ボイスクローニング、イメージクローニング、スピアフィッシング攻撃の増加を見越したもの
- 採用される生成AIソリューションは、AIのサイバーセキュリティとセキュリティ保証を確保するためのデプロイメントパイプラインのためのDevOpsとサイバーセキュリティの両方のトレーニングを含む必要があります。

### ビジネスケースの確立

しっかりとしたビジネスケースは、提案されたAIソリューションのビジネス価値を判断し、リスクとベネフィットのバランスを取り、投資対効果を評価・検証するために不可欠です。膨大な数の潜在的なユースケースがあります。

- 顧客体験の向上
- 運用効率の向上
- より良い知識管理
- イノベーションの強化
- 市場調査と競合分析
- 文書作成、翻訳、要約、分析

### ガバナンス

LLMにおけるコーポレート・ガバナンスは、組織に透明性と説明責任を提供するために必要です。テクノロジーやビジネスで選択されたユースケースに精通している可能性のあるAIプラットフォームやプロセスのオーナーを特定することは、助言されるだけでなく、十分に確立された企業のデジタルプロセスへの付随的な損害を防ぐ適切な反応速度を確保するためにも必要です。

- 組織のAI RACIチャート（責任者、説明責任者、相談者、報告者）の確立
- AIリスク、リスク評価、組織内のガバナンス責任を文書化し、割り当てます。
- データの分類と使用制限に関する、技術的な実施を含むデータ管理ポリシーを確立します。モデルは、システムのあらゆるユーザの最小アクセス・レベルに分類されたデータのみを活用すべきです。例えば、データ保護ポリシーを更新して、保護されたデータや機密データをビジネス管理されていないツールに入力しないことを強調します。
- 確立されたポリシー（例：善行基準、データ保護、ソフトウェア使用）に支えられたAIポリシーの作成
- 従業員が使用できるように、さまざまな生成AIツールの使用許容マトリクスを公開します。
- 組織が LLM 生成モデルから使用するデータのソースと管理を文書化。

### 法務

AIの法的影響の多くは未確定であり、非常に大きなコストがかかる可能性があります。IT、セキュリティ、法務のパートナーシップは、ギャップを特定し、不明瞭な決定に対処するために不可欠です。

- 製品保証を製品開発の流れの中で明確にし、製品保証の責任者をAIに割り当てます。
- 生成AIを考慮した既存の利用規約を見直し、更新します。
- AI EULA契約書のレビュー生成AIプラットフォームのエンドユーザーライセンス契約は、ユーザープロンプトの扱い方、アウトプットの権利と所有権、データプライバシー、コンプライアンス、責任、プライバシー、アウトプットの使用方法の制限など、大きく異なります。
- AIが生成したコンテンツによる盗作、偏見の伝播、知的財産の侵害に関連する責任を組織が負うことを防ぐために、顧客向けのEULA、エンドユーザー契約を変更します。
- コード開発に使用されている既存のAI支援ツールの見直し。チャットボットがコードを書く能力は、チャットボットが製品のコードを生成するために使用されている場合、その製品に関する企業の所有権を脅かす可能性があります。例えば、生成されたコンテンツの状態や保護、生成されたコンテンツを使用する権利を誰が保持しているかが問題になる可能性があります。
- 知的財産に対するリスクの確認。チャットボットによって生成された知的財産は、著作権、商標、または特許保護の対象となる生成プロセス中に不適切に取得されたデータが使用された場合、危険にさらされる可能性があります。AI製品が侵害する素材を使用した場合、AIのアウトプットにリスクが生じ、知的財産の侵害につながる可能性があります。
- 免責条項のある契約を  見直しましょう。免責条項は、責任につながる事象の責任を、その事象についてより過失があった人、あるいはその事象を阻止する最善の可能性があった人に押し付けようとするものです。AIの提供者またはその利用者が、責任を生じさせるような出来事を引き起こしたかどうかを判断するためのガードレールを確立します。
- AIシステムに起因する潜在的な傷害や物的損害に対する賠償責任を検討します。
- 保険の見直し従来の（D&O）賠償責任保険や商業賠償責任保険は、AIの利用を完全に保護するには不十分である可能性が高いです。
- 著作権に関する問題の特定著作権には、人間のオーサーシップが必要です。LLMツールが悪用された場合、組織は剽窃、偏見の伝播、知的財産権侵害の責任を負う可能性もあります。
- 開発または提供されるサービスに関して、請負業者とAIの適切な使用に関する契約が確実に結ばれていること。
- 権利行使が問題となる可能性がある場合、または知的財産権侵害が懸念される場合、従業員または請負業者に対するジェネレーティブAIツールの使用を制限または禁止します。
- 従業員管理や雇用に使用されるソリューションの評価とAIは、差別待遇クレームや差別的影響クレームの原因となる可能性があります。
- AIソリューションが適切な同意や承認なしに機密情報を収集したり共有したりしないことを確認してください。

### 規制

EUのAI法は最初の包括的なAI法になると予想されていますが、適用されるのは早くても2025年です。EUの一般データ保護規則（GDPR）はAIを特に取り上げていませんが、データ収集、データセキュリティ、公平性と透明性、正確性と信頼性、説明責任に関する規則を含んでおり、生成AIの利用に影響を与える可能性があります。米国では、AI規制はより広範な消費者プライバシー法の中に含まれています。米国では10の州で法律が成立しているか、2023年末までに施行される予定です。

米国雇用機会均等委員会（EEOC）、消費者金融保護局（CFPB）、連邦取引委員会（FTC）、米国司法省公民権局（DOJ）などの連邦機関は、雇用の公平性を厳しく監視しています。

- 国、州、またはその他の政府固有のAIコンプライアンス要件を決定します。
- 従業員の電子的監視および雇用関連の自動意思決定システムの制限に関するコンプライアンス要件の決定（バーモント州、カリフォルニア州、メリーランド州、ニューヨーク州、ニュージャージー州）
- 顔認識とAIビデオ分析に必要な同意に関するコンプライアンス要件の決定（イリノイ州、メリーランド州、ワシントン州、バーモント州）
- 従業員の採用や管理に使用されている、または検討されているAIツールを確認します。
- ベンダーが適用されるAI法やベストプラクティスを遵守していることを確認します。
- 採用プロセスでAIを使用した製品について質問し、文書化します。モデルがどのように訓練され、どのように監視され、差別や偏見を避けるために修正されたものを追跡しているかを尋ねます。
- どのような宿泊オプションが含まれているかを尋ね、記録しておきましょう。
- ベンダーが機密データを収集しているかどうかを尋ね、文書化してください。
- ベンダーやツールがどのようにデータを保存・削除し、入社前の顔認識やビデオ分析ツールの使用を規制しているかを尋ねてください。
- コンプライアンス上の問題が発生する可能性のある他の組織固有の規制要件をAIで確認してください。例えば、1974年従業員退職所得保障法（Employee Retirement Income Security Act of 1974）には、チャットボットでは対応できない可能性のある退職金制度のための教育義務要件があります。

### 大規模言語モデルソリューションの使用または実装

- 脅威モデル LLM のコンポーネントとアーキテクチャの信頼境界。
- データ・セキュリティ、データがどのように機密性に基づいて分類され、保護されているかを検証してください。(ユーザの権限はどのように管理され、どのような保護措置が取られていますか？）
- アクセス・コントロール、最小権限アクセス・コントロールの実施、徹底的な防衛策の実施
- トレーニングパイプラインセキュリティは、トレーニングデータガバナンス、パイプライン、モデル、アルゴリズムに関する厳格な管理を必要とします。
- 入力と出力のセキュリティでは、入力の検証方法と、出力がどのようにフィルタリングされ、サニタイズされ、承認されるかを評価します。
- 監視と応答、ワークフロー、監視、応答をマッピングし、自動化、ロギング、監査を理解します。監査記録の安全性を確認します。
- 本番リリースプロセスにおけるアプリケーションテスト、ソースコードレビュー、脆弱性評価、レッドチーミングの実施。
- LLMモデルまたはサプライチェーンに既存の脆弱性がないかチェックします。
- プロンプト・インジェクション、機密情報の流出、プロセス操作など、LLMソリューションに対する脅威や攻撃の影響を調査します。
- モデルポイズニング、不適切なデータの取り扱い、サプライチェーン攻撃、モデルの盗難など、LLMモデルに対する攻撃や脅威の影響を調査します。
- サプライチェーンセキュリティ、第三者監査、侵入テスト、第三者プロバイダーのコードレビューを依頼します。(初期および継続的に）
- インフラストラクチャ・セキュリティ・ベンダーがレジリエンス・テストを実施する頻度は？可用性、スケーラビリティ、パフォーマンスに関するSLAはどうなっていますか。
- インシデント対応プレイブックを更新し、卓上演習にLLMインシデントを盛り込みます。
- 生成的なサイバーセキュリティAIを他のアプローチと比較するためのベンチマーク指標を特定または拡張 期待される生産性の向上を測定します。

### 試験、評価、検証、妥当性確認（TEVV）

NIST AIフレームワークでは、AIシステム運用者、ドメインエキスパート、AI設計者、ユーザー、製品開発者、評価者、監査人を含む、AIライフサイクル全体にわたる継続的なTEVVプロセスを推奨しています。TEVVには、システムの妥当性確認、統合、テスト、再較正、AIシステムのリスクや変更をナビゲートするための定期的な更新のための継続的なモニタリングなど、さまざまなタスクが含まれます。

- AIモデルのライフサイクルを通じて、継続的なテスト、評価、検証、妥当性確認を確立します。
- AIモデルの機能性、セキュリティ、信頼性、堅牢性に関する定期的な経営指標と最新情報の提供。

### モデル・カードとリスク・カード

モデルカードとリスクカードは、大規模言語モデル（LLM）の透明性、説明責任、および倫理的な導入を向上させるための基本要素です。モデル・カードは、AIシステムの設計、能力、制約に関する標準化されたドキュメントを提供することで、ユーザがAIシステムを理解し、信頼できるようにします。リスクカードは、バイアス、プライバシーの問題、セキュリティの脆弱性など、潜在的な悪影響をオープンに扱うことでこれを補い、危害防止への積極的なアプローチを促します。これらの文書は、AIśの社会的影響が慎重に扱われ、対処される協力的な雰囲気を確立するため、開発者、ユーザー、規制当局、倫理学者にとって等しく重要です。これらのカードは、モデルを作成した組織によって開発され、維持されており、AI技術が倫理基準と法的要件を満たすことを保証し、AIエコシステムにおける責任ある研究と展開を可能にする上で重要な役割を果たしています。

モデルカードには、MLモデルに関連する主要な属性が含まれています：

  - モデルの詳細：モデルに関する基本情報、すなわち、名前、バージョン、タイプ（ニューラルネットワーク、決定木など）、および意図されたユースケース。
  - モデルのアーキテクチャ：層の数とタイプ、活性化関数、その他の主要なアーキテクチャの選択など、モデルの構造の説明を含みます。
  - トレーニングデータと手法：データセットのサイズ、データソース、使用した前処理やデータ補強技術など、モデルの学習に使用したデータに関する情報。また、使用されたオプティマイザ、損失関数、チューニングされたハイパーパラメータなど、トレーニング手法の詳細も含まれます。
  - パフォーマンス測定基準：精度、正確度、再現率、F1スコアなど、さまざまな測定基準におけるモデルのパフォーマンスに関する情報。また、データの異なるサブセットでのモデルのパフォーマンスに関する情報も含まれます。
  - 潜在的なバイアスと限界：不均衡なトレーニングデータ、オーバーフィッティング、モデルの予測におけるバイアスなど、モデルの潜在的なバイアスや制限をリストアップします。また、新しいデータへの汎化能力や特定のユースケースへの適合性など、モデルの限界に関する情報も含まれます。
  - 責任あるAIへの配慮：プライバシーに関する懸念、公平性、透明性、またはモデルの使用による潜在的な社会的影響など、モデルに関連する倫理的または責任あるAIの考慮事項。また、モデルのさらなるテスト、検証、モニタリングに関する推奨事項が含まれる場合もあります。

モデルカードに含まれる正確な機能は、モデルのコンテキストと使用目的によって異なる可能性がありますが、その目的は、機械学習モデルの作成と展開に公開性と説明責任を与えることです。

- モデルカードのレビュー
- リスクカードがある場合はそれを確認
- サードパーティを通じて使用されるモデルを含む、あらゆる導入モデルのモデルカードを追跡し、維持するプロセスを確立します。

### RAG：大規模言語モデルの最適化

ファインチューニングは、事前に訓練されたモデルを最適化するための伝統的な手法で、既存のモデルを新しいドメイン固有のデータで再学習させ、タスクやアプリケーションのパフォーマンスに合わせて修正します。ファインチューニングにはコストがかかりますが、パフォーマンス向上には不可欠です。

検索補強型生成（RAG）は、最新の利用可能な知識ソースから適切なデータを検索することにより、大規模な言語モデルの能力を最適化し補強する、より効果的な方法として発展してきました。RAGは特定のドメイン用にカスタマイズすることができ、ドメイン固有の情報の検索を最適化し、特殊な分野のニュアンスに合わせて生成プロセスを調整します。RAGは、LLM最適化のための、より効率的で透明性の高い手法と考えられており、特にラベル付きデータの収集が限られていたり、高価であったりする問題に適しています。RAGの主な利点の一つは、検索段階で新しい情報を継続的に更新することができるため、継続的な学習をサポートすることです。

RAGの実装には、埋め込みモデルの展開から始まり、知識ライブラリの索引付け、クエリ処理のための最も関連性の高い文書の検索まで、いくつかの重要なステップが含まれます。関連するコンテキストの効率的な検索は、文書埋め込みモデルの保存とクエリに使用されるベクトルデータベースに基づいて行われます。

#### RAG 参照・リンク
- [検索拡張世代（RAG）とLLM：例](https://vitalflux.com/retrieval-augmented-generation-rag-llm-examples/)
- [12 RAGの問題点と解決案](https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c)

### AIレッドチーム

AIレッドチームとは、攻撃者が悪用できる脆弱性が存在しないことを確認するために、AIシステムを敵対的に攻撃するテストシミュレーションです。これは、バイデン政権を含む多くの規制機関やAI管理機関によって推奨されています。レッドチーミングだけでは、AIシステムに関連するすべての実害を検証する包括的な解決策とはならず、アルゴリズムによる影響評価や外部監査など、他の形式のテスト、評価、検証、妥当性確認と組み合わせる必要があります。

- AIモデルとアプリケーションの標準的なプラクティスとして、レッドチームテストを導入します。