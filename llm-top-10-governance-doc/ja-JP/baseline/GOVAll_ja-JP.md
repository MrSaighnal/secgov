## 第一章　概要

すべてのインターネットユーザーと企業は、来るべき強力な生成的人工知能（生成AI）アプリケーションの波に備えなければなりません。生成AIは、様々な業界に、革新と効率化、商業的成功をもたらす大きな可能性を秘めている一方、他の強力な初期段階のテクノロジーと同様、予期せぬ、新たな課題ももたらすからです。

人工知能は、過去50年間大きく発展し、目立たないかたちで、企業の様々なプロセスをゆるやかにサポートしてきましたが、ChatGPTの登場により、個人のレベル、並びに企業の間で、大規模言語モデル(LLM)の開発と利用が推し進められることとなりました。　当初、これらのテクノロジーは、学術的な研究や、企業内の特定の、しかし重要な活動の実行に限定され、一部の人にしか見えませんでした。しかし、データの可用性、コンピュータの能力、生成AIの能力、そしてLlama 2、ElevenLabs、Midjourneyのようなツールのリリースにおける最近の進歩は、AIをニッチなものから一般的に広く受け入れられるものへと引き上げました。これらの改善は、生成AI技術をより身近なものにしただけでなく、企業が業務にAIを統合し、活用するための確固たる戦略を開発する必要性を浮き彫りにし、テクノロジーの利用方法における大きな前進を表しています。

人工知能(AI)は、通常人間の知能を必要とするタスクを機械が達成できるようにするコンピュータサイエンスのすべての分野を包含する広い用語です。機械学習と生成AIはAIの2つのサブカテゴリーです。
機械学習はAIのサブセットで、データから学習できるアルゴリズムの作成に重点を置いています。機械学習アルゴリズムは一連のデータで学習され、そのデータを使って新しいデータの予測や意思決定を行うことができます。
生成AIは、新しいデータの作成に焦点を当てた機械学習の一種です。
大規模言語モデル（LLM）は、人間のようなテキストを処理して生成するAIモデルの一種です。人工知能の文脈で「モデル」とは、入力データに基づいて予測を行うように訓練されたシステムを指します。LLMは特に自然言語の大規模なデータセットで学習されるため、大規模言語モデルと呼ばれています。

組織は生成AIソリューションのセキュリティ確保と監督において未知の領域に突入しています。生成AIの急速な進歩は、敵対者が攻撃戦略を強化する門戸も開き、防御と脅威の拡大という二重の課題をもたらします。

企業は、採用のための人事、電子メールのスパムスクリーニング、行動分析のためのSIEM、管理された検出と応答のアプリケーションなど、多くの分野で人工知能を使用しています。しかし、このドキュメントの主な焦点は、大規模言語モデルのアプリケーションと、 生成されたコンテンツを作成する機能です。

### 責任ある信頼できる人工知能

人工知能の課題と利点が明らかになり、規制や法律が成立するにつれ、責任ある信頼できる AI 利用の原則と柱は、理想主義的な対象や懸念から確立された基準へと進化しています。OWASP AI Exchange ワーキンググループは、このような変化を監視し、人工知能のあらゆる側面における、より広範でより困難な考慮事項に取り組んでいます。

信頼できる AI の条件

確実性（reliable）
　　　　強靭であること（robust）
　　　　責任の所在が明らかなこと（accountable）
　　　　常に確認していること（monitored）
　　　　透明性があること（transparent）
　　　　なぜかを説明できること（explainable）

安全性（resilience）
　　　　安全であること（safe）
　　　　安心できること（secure）
　　　　個人情報が守られていること（private）
　　　　効果的であること（effective）

道義性（responsible）
　　　　公正であること（fair）
　　　　道徳的に正しいこと（ethical）
　　　　一部の人をのけ者にしないこと（inclusive）
　　　　持続可能なこと（sustainable）
　　　　はっきりした目的があること（purposeful）

##### 図 1.1　信頼できる AI の条件

### 対象読者

OWASP 大規模言語モデル アプリケーション リスク トップ10 サイバーセキュリティとガバナンスのチェックリストは、経営幹部、技術者、サイバーセキュリティ、プライバシー、コンプライアンス、法務、DevSecOps、MLSecOps、サイバーセキュリティチーム、および守備担当者の各分野のリーダーを対象としています。AIを企業の成功に活用するだけでなく、性急で安全でないAIの実装によるリスクからも保護することを目指し、急速に変化するAIの世界で一歩先を行く努力をしている人々を対象としています。こうしたリーダーやチームは、チャンスをつかみ、課題と戦い、リスクを軽減するための戦術を構築しなければなりません。

このチェックリストは、技術およびビジネスリーダーがLLMを使用するリスクとメリットを迅速に理解し、大規模言語モデル戦略を策定する際に、組織を防御し保護するために必要な重要な領域とタスクの包括的なリストの作成に集中できるようにすることを目的としています。

OWASP 大規模言語モデル アプリケーション リスク トップ10 チームは、このリストが組織の既存の防御テクニックの改善や、このエキサイティングなテクノロジーを使用することで新たに発生する脅威（ ）に対処するテクニックの開発に役立つことを願っています。

### なぜチェックリストが必要なのか

戦略を策定するために使用されるチェックリストは、正確性を向上させ、目的を明確にし、統一性を保ち、集中した慎重な作業を促し、見落としや細部の見逃しを減らします。チェックリストに従うことは、安全な採用の旅への信頼を高めるだけでなく、継続的な改善のためのシンプルで効果的な戦略を提供することで、将来の組織の革新を促します。

### 包括的ではない

本文書は、急速に変化する技術的、法的、及び規制的環境において、組織が初期のLLM戦略を策定する際の支援を意図していますが、網羅的なものではなく、全てのユースケースや義務を網羅するものではあり ません。組織は、この文書を使用する一方で、提供されたチェックリストの範囲を超えて、その使用 ケース又は管轄区域の必要に応じて、評価及び実務を拡張する必要があります。

### 大規模言語モデルの課題

大規模な言語モデルは、いくつかの深刻で独特な問題に直面しています。最も重要なことの1つは、LLMで作業している間、制御プレーンとデータプレーンを厳密に分離または分離することができないということです。もう1つの重大な課題は、LLMは設計上非決定論的であり、促したり要求したりすると異なる結果をもたらすことです。LLMはキーワード検索ではなくセマンティック検索を採用しています。この2つの重要な違いは、モデルのアルゴリズムがレスポンスに含まれる用語に優先順位をつけることです。これは消費者がこれまでテクノロジーを使ってきた方法とは大きく異なり、結果の一貫性と信頼性に影響を与えます。幻覚は、モデルがトレーニングされたデータのギャップやトレーニングの欠陥から生じるもので、この方法の結果です。

信頼性を向上させ、ジェイルブレイク、モデルトリック、幻覚に対する攻撃面を減らす方法はありますが、コストと機能の両方において、制限と実用性の間にトレードオフがあります。

LLM の使用と LLM アプリケーションは、組織の攻撃対象領域を拡大します。関連するリスク
 
LLMはユニークですが、既知のソフトウェア部品表（SBoM）、サプライチェーン、データ損失保護（DLP）、認証されたアクセスなど、多くはおなじみの問題です。また、生成AIとは直接関係のないリスクの増加もありますが、生成AIは組織を攻撃し脅かす攻撃者の効率性、能力、有効性を高めます。

LLM とジェネレーティブ AI ツールを活用し、組織、個人、政府システムを攻撃する従来の手法を見直し、迅速化する動きが加速しています。LLMは、新しいゼロデイ脆弱性を埋め込んだり、検知を回避するように設計された新しいマルウェアを簡単に作成できるようにするテクニックを強化する能力を促進します。また、洗練されたユニークなフィッシング詐欺の手口を生み出すことも可能です。動画や音声を使った説得力のあるフェイクを作成することで、ソーシャルエンジニアリングの策略をさらに促進します。さらに、これらのツールは、侵入を実行し、革新的なハッキング能力を開発することを可能にします。今後、犯罪行為者によるAI技術の「テーラーメイド」かつ複合的な利用が進むにつれ、組織の適切な防御・回復能力に対する特別な対応や専用ソリューションが求められるようになるでしょう。

組織はまた、競争上の不利、顧客やパートナーからの時代遅れという市場認識、パーソナライズされたコミュニケーションの拡張不能、イノベーションの停滞、運用上の不備、プロセスにおける人為的ミスのリスクの増大、人的資源の不適切な配分など、LLMの能力を活用できない脅威にも直面しています。

さまざまな種類の脅威を理解し、ビジネス戦略と統合することで、大規模言語モデル（LLM）を使用するメリットとデメリットを比較検討することができます。 、ビジネス目標の達成を妨げるのではなく、むしろ加速させることができます。

### AI の脅威

AI の脅威の分類

AI モデルを使わないことによる脅威
AI 法的な規制による脅威
AI モデルを使うことによる脅威
AI モデルが狙われることによる脅威
AI モデルが生み出す脅威

##### 図 1.2　AIの脅威の分類

### 人工知能のセキュリティとプライバシーのトレーニング

人工知能（AI）、生成AI、LLMの構築、購入、利用が将来もたらす可能性のある結果を理解するためのトレーニングは、組織全体の従業員にとって有益です。許容される使用とセキュリティ意識に関するトレーニングは、すべての従業員を対象とするだけでなく、人事部、法務部、開発者、データチーム、セキュリティチームなど、特定の職種に特化したものであるべきです。

公正使用ポリシーと健全な相互作用は、最初から組み込まれていれば、将来のAIサイバーセキュリティ啓発キャンペーンの成功の礎となる重要な側面です。これにより、ユーザーには対話のための基本的なルールの知識だけでなく、 、良い行動と悪い行動や非倫理的な行動を区別する能力が提供されることになります。

### LLMのセキュリティとガバナンスを既存の確立された実務と
### 統制に組み込むこと

AIと生成されたAIは、サイバーセキュリティ、回復力、プライバシー、法的・規制的要件への対応に新たな局面をもたらしますが、問題を特定し、脆弱性を発見し、脆弱性を修正し、潜在的なセキュリティ問題を軽減するには、以前からあるベストプラクティスが依然として最善の方法です。

人工知能システムの管理が既存の組織慣行と統合されていることを確認します。
AI ML システムが既存のプライバシー、ガバナンス、およびセキュリティ慣行に従うことを確認し、AI が要求する場合には、特定のプライバシー、ガバナンス、およびセキュリティ慣行を実施します。

### セキュリティの基本原則

LLM の機能は、異なるタイプの攻撃と攻撃対象領域をもたらします。LLMは、プロンプト・インジェクション、安全でないプラグイン設計、リモート・コード実行などの複雑なビジネス・ロジックのバグに対して脆弱です。これらの問題を解決するには、既存のベストプラクティスが最適です。セキュアなソフトウェアのレビュー、アーキテクチャ、データガバナンス、サードパーティの評価を理解している社内の製品セキュリティチーム サイバーセキュリティチームは、音声クローニング、なりすまし、キャプチャのバイパスなど、LLMによって悪化する可能性のある問題を見つけるために、現在のコントロールがどの程度強力であるかもチェックする必要があります。機械学習、NLP（自然言語処理）、NLU（自然言語理解）、ディープラーニング、そして最近ではLLM（大規模言語モデル）やジェネレーティブAIの最近の進歩を考慮すると、サイバーセキュリティとデブオプスのチームと一緒にこれらの分野に精通した専門家を含めることをお勧めします。彼らの専門知識は、これらの技術を採用するだけでなく、新たな課題に対する革新的な分析や対応を開発する上でも役立ちます。

### リスク

リスクに関する言及はISO31000の定義を使用：リスク＝「目的に対する不確実性の影響」。チェックリストに含まれるLLMリスクには、敵対的リスク、安全リスク、 法務リスク、規制リスク、レピュテーションリスク、財務リスク、競争リスクに対応するLLMリスクのターゲットリストが含まれます。

### 脆弱性と緩和の分類法

OVAL、STIX、CVE、CWEなど、脆弱性を分類し脅威情報を共有するための現在のシステムは、大規模言語モデル（LLM）や予測モデルに固有の脆弱性や脅威を監視し、防御者に警告する機能を開発中です。AI/MLシステムとそのサプライチェーンに対する脆弱性や脅威が特定された場合、組織は脆弱性分類のためのCVEやサイバー脅威インテリジェンス（CTI）の交換のためのSTIXのような、確立され認知されたこれらの標準に依存することが予想されます。
## 第二章　LLM戦略の決定

大規模言語モデル（Large Language Model：LLM）アプリケーションの急速な拡大により、業務で使用されるすべてのAI/MLシステム（Generative AI（ジェネレーティブAI）および古くから確立されているPredictive AI/MLシステムの両方を含む）への注目と検討が高まっています。このような注目の高まりは、これまで見過ごされていたシステムを標的とする攻撃者や、法的、プライバシー、責任、保証の観点から軽視されていたガバナンスや法的課題などの潜在的なリスクを露呈しています。AI/MLシステムを業務に活用するあらゆる組織にとって、包括的なポリシー、ガバナンス、セキュリティ・プロトコル、プライバシー対策、アカウンタビリティ基準を評価・確立し、これらのテクノロジーが安全かつ倫理的にビジネス・プロセスと整合していることを確認することが重要です。

攻撃者（敵対者）は、企業、国民、政府機関にとって最も直接的で有害な脅威です。彼らの目的は、金銭的な利益からスパイ活動まで多岐にわたり、重要な情報を盗み、業務を妨害し、信用を傷つけることにあります。さらに、AIや機械学習などの新技術を活用する彼らの能力は、攻撃のスピードと洗練度を高め、防御が攻撃に先んじることを困難にしています。

多くの組織にとって最も差し迫った逆らえないLLMの脅威は、「シャドーAI」によるものです。従業員が未承認のオンラインAIツール、安全でないブラウザのプラグイン、標準的なソフトウェアの承認プロセスを回避してアップデートやアップグレードによってLLM機能を導入するサードパーティのアプリケーションを使用している場合です。

LLM Deployment Steps

Step 1: Resilience First Strategy

▶ Identify immediate threads by Threat Modeling abuse cases
▶ Review internal or external exploitation cases for threat model scenarios and verify security controls
▶ Scan and monitor the environment for any instances of roge applications

Step 2: Update existing policy

▶ Review contracts, NDA, governance, and security to incorporate LLM or GenAI use or threat

Step 3: Training / Education

▶ Update security awareness training, developer, legal, or other training to include LLM or GenAI use or threats

Step 4: Engage with leaders

▶ Work with executives, business leaders, and other stakeholders to identify an LLM or GenAI solutions strategy
▶ Implement mitigation / risk strategy

Step 5: Update Third Party Risk Management Program

▶ Third-party and vendor AI solutions will require augmented questionnaires and reviews

Step 6: Choose a development strategy

##### 図 2.1　展開戦略の選択肢

### 展開戦略

その範囲は、一般消費者向けのアプリケーションを活用するものから、個人データで独自のモデルをトレーニングするものまで多岐にわたります。使用ケースの感度、必要な機能、利用可能なリソースなどの要因が、利便性と制御の適切なバランスを決定するのに役立ちます。しかし、これらの5つのモデルタイプを理解することは、オプションを評価するためのフレームワークを提供します。

展開

タイプ 1: 直接、アクセスする

▶ 自社製の大規模言語モデルのインターフェースを使う
▶ 会社の方針を決め、従業員のトレーニングを行って、リスクを軽減する
▶ 利点：柔軟に、早く実験を進めることができる
▶ 例：Perplexity, ChatGPT, big-AGI

タイプ 2: モデルAPIを経由してアクセスする

▶ ベンダー提供の大規模言語モデルのAPIを使いアクセスする
▶ 会社の方針を決め、従業員のトレーニングを行って、リスクを軽減する
▶ 利点：使用するAPIを
▶ 例：Perplexity, ChatGPT, big-AGI


Type 3: Pre-Trained Models

▶ Start with a general foundation model, then customize by fine-tuning on your data
▶ Accelerates timelines versus building fully custom.  Examples: Falcon, Llama, and Amazon Bedrock

Type 4: Fine-Tuned Models

▶ Start with proven models and tine-tune further with proprietary data to adapt them to your domain
▶ Enables customization beyond pre-trained, Amazon SageMaker, Amazon Bedrock, Llama2, LegalAI

Type 5: Custom Models

▶ Build tailored architectures from scratch for your specific use case
▶ Requires large investment but maximizes customization

##### 図 2.2　展開タイプ
##### 作成: sdunn
## 第三章　チェックリスト

### 敵対的リスク

敵対的リスクには、競合他社や攻撃者が含まれます。

競合他社がどのように人工知能に投資しているかを精査してください。AIの導入にはリスクがありますが、将来の市場ポジションに影響を与えるビジネス上のメリットもあります
音声認識を利用したパスワードのリセットなど、新たな生成AIによる攻撃から身を守るための適切なセキュリティがもはや提供されていない可能性のある現行の管理策の影響を調査します。
インシデント・レスポンス・プランとプレイブックを更新し、生成AI拡張攻撃とAIML 特有のインシデントに対応。

### 脅威のモデリング

脅威を特定し、プロセスとセキュリティ防御を検討するために、脅威モデリングを強く推奨します。脅威モデリングは、アプリケーション、ソフトウェア、システムに対して合理的なセキュリティ決定を行うことを可能にする、体系的で反復可能な一連のプロセスです。生成AIが加速する攻撃やLLMを導入する前の脅威モデリングは、リスクを特定し、軽減し、データを保護し、プライバシーを保護し、ビジネス内の安全でコンプライアンスに準拠した統合を確実にするための最も費用対効果の高い方法です。

攻撃者は、組織、従業員、経営陣、ユーザーに対する攻撃をどのように加速させるのでしょうか？組織は、Generative AIを使用した大規模な「ハイパーパーソナライズド」攻撃を予測する必要があります。LLMが支援するスピア・フィッシング攻撃は、今や指数関数的に効果的で、標的が絞られ、攻撃の武器となります。
生成AIは、スプーフィングや生成AIが生成したコンテンツを通じて、企業の顧客やクライアントへの攻撃にどのように利用される可能性がありますか？
LLMソリューションへの有害または悪意のある入力やクエリを検出し、無効化できますか？
LLMのすべての信頼境界において、安全な統合により既存のシステムやデータベースとの接続を保護できますか？
許可されたユーザーによる悪用を防ぐために、内部脅威の緩和策を講じていますか？
知的財産を保護するために、独自のモデルやデータへの不正アクセスを防止できますか？
コンテンツフィルタリングを自動化することで、有害または不適切なコンテンツの生成を防ぐことができますか？

### AI資産目録

AI資産目録は、社内で開発されたソリューションと外部またはサードパーティのソリューションの両方に適用する必要があります。

既存のAIサービス、ツール、所有者をカタログ化します。資産管理で特定のインベントリのタグを指定します。
ソフトウェア部品表（SBOM）は、アプリケーションに関連するすべてのソフトウェア部品、依存関係、およびメタデータの包括的なリストです。
AIデータソースのカタログとデータの機密性（保護、機密、公開）
現在の攻撃サーフェスのリスクを判断するために、配備されたAIソリューションのペンテストまたはレッドチームが必要かどうかを確認します。
AIソリューションのオンボーディング・プロセスの構築
SBoM の要件に従い、熟練したIT管理スタッフを社内または社外から確保します。

### AIセキュリティ・プライバシー研修

従業員と積極的に関わり、LLMの計画的な取り組みについて理解し、懸念に対処します。
組織のプロセス、システム、従業員の管理とサポート、顧客との関係における予測的または生成的なAIの使用と、その使用がどのように管理され、リスク対処されるかについて、オープンで透明性のあるコミュニケーション文化を確立すること。
倫理、責任、および保証、ライセンス、著作権などの法的問題について、すべてのユーザーを訓練します。
生成AI関連の脅威を含むセキュリティ意識向上トレーニングの更新。ボイスクローニング、イメージクローニング、スピアフィッシング攻撃の増加を見越したもの
採用される生成AIソリューションは、AIのサイバーセキュリティとセキュリティ保証を確保するためのデプロイメントパイプラインのためのDevOpsとサイバーセキュリティの両方のトレーニングを含む必要があります。

### ビジネスケースの確立

しっかりとしたビジネスケースは、提案されたAIソリューションのビジネス価値を判断し、リスクとベネフィットのバランスを取り、投資対効果を評価・検証するために不可欠です。膨大な数の潜在的なユースケースがあります。

顧客体験の向上
運用効率の向上
より良い知識管理
イノベーションの強化
市場調査と競合分析
文書作成、翻訳、要約、分析

### ガバナンス

LLMにおけるコーポレート・ガバナンスは、組織に透明性と説明責任を提供するために必要です。テクノロジーやビジネスで選択されたユースケースに精通している可能性のあるAIプラットフォームやプロセスのオーナーを特定することは、助言されるだけでなく、十分に確立された企業のデジタルプロセスへの付随的な損害を防ぐ適切な反応速度を確保するためにも必要です。

組織のAI RACIチャート（責任者、説明責任者、相談者、報告者）の確立
AIリスク、リスク評価、組織内のガバナンス責任を文書化し、割り当てます。
データの分類と使用制限に関する、技術的な実施を含むデータ管理ポリシーを確立します。モデルは、システムのあらゆるユーザの最小アクセス・レベルに分類されたデータのみを活用すべきです。例えば、データ保護ポリシーを更新して、保護されたデータや機密データをビジネス管理されていないツールに入力しないことを強調します。
確立されたポリシー（例：善行基準、データ保護、ソフトウェア使用）に支えられたAIポリシーの作成
従業員が使用できるように、さまざまな生成AIツールの使用許容マトリクスを公開します。
組織が LLM 生成モデルから使用するデータのソースと管理を文書化。

### 法務

AIの法的影響の多くは未確定であり、非常に大きなコストがかかる可能性があります。IT、セキュリティ、法務のパートナーシップは、ギャップを特定し、不明瞭な決定に対処するために不可欠です。

製品保証を製品開発の流れの中で明確にし、製品保証の責任者をAIに割り当てます。
生成AIを考慮した既存の利用規約を見直し、更新します。
AI EULA契約書のレビュー生成AIプラットフォームのエンドユーザーライセンス契約は、ユーザープロンプトの扱い方、アウトプットの権利と所有権、データプライバシー、コンプライアンス、責任、プライバシー、アウトプットの使用方法の制限など、大きく異なります。
AIが生成したコンテンツによる盗作、偏見の伝播、知的財産の侵害に関連する責任を組織が負うことを防ぐために、顧客向けのEULA、エンドユーザー契約を変更します。
コード開発に使用されている既存のAI支援ツールの見直し。チャットボットがコードを書く能力は、チャットボットが製品のコードを生成するために使用されている場合、その製品に関する企業の所有権を脅かす可能性があります。例えば、生成されたコンテンツの状態や保護、生成されたコンテンツを使用する権利を誰が保持しているかが問題になる可能性があります。
知的財産に対するリスクの確認。チャットボットによって生成された知的財産は、著作権、商標、または特許保護の対象となる生成プロセス中に不適切に取得されたデータが使用された場合、危険にさらされる可能性があります。AI製品が侵害する素材を使用した場合、AIのアウトプットにリスクが生じ、知的財産の侵害につながる可能性があります。
免責条項のある契約を  見直しましょう。免責条項は、責任につながる事象の責任を、その事象についてより過失があった人、あるいはその事象を阻止する最善の可能性があった人に押し付けようとするものです。AIの提供者またはその利用者が、責任を生じさせるような出来事を引き起こしたかどうかを判断するためのガードレールを確立します。
AIシステムに起因する潜在的な傷害や物的損害に対する賠償責任を検討します。
保険の見直し従来の（D&O）賠償責任保険や商業賠償責任保険は、AIの利用を完全に保護するには不十分である可能性が高いです。
著作権に関する問題の特定著作権には、人間のオーサーシップが必要です。LLMツールが悪用された場合、組織は剽窃、偏見の伝播、知的財産権侵害の責任を負う可能性もあります。
開発または提供されるサービスに関して、請負業者とAIの適切な使用に関する契約が確実に結ばれていること。
権利行使が問題となる可能性がある場合、または知的財産権侵害が懸念される場合、従業員または請負業者に対するジェネレーティブAIツールの使用を制限または禁止します。
従業員管理や雇用に使用されるソリューションの評価とAIは、差別待遇クレームや差別的影響クレームの原因となる可能性があります。
AIソリューションが適切な同意や承認なしに機密情報を収集したり共有したりしないことを確認してください。

### 規制

EUのAI法は最初の包括的なAI法になると予想されていますが、適用されるのは早くても2025年です。EUの一般データ保護規則（GDPR）はAIを特に取り上げていませんが、データ収集、データセキュリティ、公平性と透明性、正確性と信頼性、説明責任に関する規則を含んでおり、生成AIの利用に影響を与える可能性があります。米国では、AI規制はより広範な消費者プライバシー法の中に含まれています。米国では10の州で法律が成立しているか、2023年末までに施行される予定です。

米国雇用機会均等委員会（EEOC）、消費者金融保護局（CFPB）、連邦取引委員会（FTC）、米国司法省公民権局（DOJ）などの連邦機関は、雇用の公平性を厳しく監視しています。

国、州、またはその他の政府固有のAIコンプライアンス要件を決定します。
従業員の電子的監視および雇用関連の自動意思決定システムの制限に関するコンプライアンス要件の決定（バーモント州、カリフォルニア州、メリーランド州、ニューヨーク州、ニュージャージー州）
顔認識とAIビデオ分析に必要な同意に関するコンプライアンス要件の決定（イリノイ州、メリーランド州、ワシントン州、バーモント州）
従業員の採用や管理に使用されている、または検討されているAIツールを確認します。
ベンダーが適用されるAI法やベストプラクティスを遵守していることを確認します。
採用プロセスでAIを使用した製品について質問し、文書化します。モデルがどのように訓練され、どのように監視され、差別や偏見を避けるために修正されたものを追跡しているかを尋ねます。
どのような宿泊オプションが含まれているかを尋ね、記録しておきましょう。
ベンダーが機密データを収集しているかどうかを尋ね、文書化してください。
ベンダーやツールがどのようにデータを保存・削除し、入社前の顔認識やビデオ分析ツールの使用を規制しているかを尋ねてください。
コンプライアンス上の問題が発生する可能性のある他の組織固有の規制要件をAIで確認してください。例えば、1974年従業員退職所得保障法（Employee Retirement Income Security Act of 1974）には、チャットボットでは対応できない可能性のある退職金制度のための教育義務要件があります。

### 大規模言語モデルソリューションの使用または実装

脅威モデル LLM のコンポーネントとアーキテクチャの信頼境界。
データ・セキュリティ、データがどのように機密性に基づいて分類され、保護されているかを検証してください。(ユーザの権限はどのように管理され、どのような保護措置が取られていますか？）
アクセス・コントロール、最小権限アクセス・コントロールの実施、徹底的な防衛策の実施
トレーニングパイプラインセキュリティは、トレーニングデータガバナンス、パイプライン、モデル、アルゴリズムに関する厳格な管理を必要とします。
入力と出力のセキュリティでは、入力の検証方法と、出力がどのようにフィルタリングされ、サニタイズされ、承認されるかを評価します。
監視と応答、ワークフロー、監視、応答をマッピングし、自動化、ロギング、監査を理解します。監査記録の安全性を確認します。
本番リリースプロセスにおけるアプリケーションテスト、ソースコードレビュー、脆弱性評価、レッドチーミングの実施。
LLMモデルまたはサプライチェーンに既存の脆弱性がないかチェックします。
プロンプト・インジェクション、機密情報の流出、プロセス操作など、LLMソリューションに対する脅威や攻撃の影響を調査します。
モデルポイズニング、不適切なデータの取り扱い、サプライチェーン攻撃、モデルの盗難など、LLMモデルに対する攻撃や脅威の影響を調査します。
サプライチェーンセキュリティ、第三者監査、侵入テスト、第三者プロバイダーのコードレビューを依頼します。(初期および継続的に）
インフラストラクチャ・セキュリティ・ベンダーがレジリエンス・テストを実施する頻度は？可用性、スケーラビリティ、パフォーマンスに関するSLAはどうなっていますか。
インシデント対応プレイブックを更新し、卓上演習にLLMインシデントを盛り込みます。
生成的なサイバーセキュリティAIを他のアプローチと比較するためのベンチマーク指標を特定または拡張 期待される生産性の向上を測定します。

### 試験、評価、検証、妥当性確認（TEVV）

NIST AIフレームワークでは、AIシステム運用者、ドメインエキスパート、AI設計者、ユーザー、製品開発者、評価者、監査人を含む、AIライフサイクル全体にわたる継続的なTEVVプロセスを推奨しています。TEVVには、システムの妥当性確認、統合、テスト、再較正、AIシステムのリスクや変更をナビゲートするための定期的な更新のための継続的なモニタリングなど、さまざまなタスクが含まれます。

AIモデルのライフサイクルを通じて、継続的なテスト、評価、検証、妥当性確認を確立します。
AIモデルの機能性、セキュリティ、信頼性、堅牢性に関する定期的な経営指標と最新情報の提供。

### モデル・カードとリスク・カード

モデルカードとリスクカードは、大規模言語モデル（LLM）の透明性、説明責任、および倫理的な導入を向上させるための基本要素です。モデル・カードは、AIシステムの設計、能力、制約に関する標準化されたドキュメントを提供することで、ユーザがAIシステムを理解し、信頼できるようにします。リスクカードは、バイアス、プライバシーの問題、セキュリティの脆弱性など、潜在的な悪影響をオープンに扱うことでこれを補い、危害防止への積極的なアプローチを促します。これらの文書は、AIśの社会的影響が慎重に扱われ、対処される協力的な雰囲気を確立するため、開発者、ユーザー、規制当局、倫理学者にとって等しく重要です。これらのカードは、モデルを作成した組織によって開発され、維持されており、AI技術が倫理基準と法的要件を満たすことを保証し、AIエコシステムにおける責任ある研究と展開を可能にする上で重要な役割を果たしています。

モデルカードには、MLモデルに関連する主要な属性が含まれています：

モデルの詳細：モデルに関する基本情報、すなわち、名前、バージョン、タイプ（ニューラルネットワーク、決定木など）、および意図されたユースケース。
モデルのアーキテクチャ：層の数とタイプ、活性化関数、その他の主要なアーキテクチャの選択など、モデルの構造の説明を含みます。
トレーニングデータと手法：データセットのサイズ、データソース、使用した前処理やデータ補強技術など、モデルの学習に使用したデータに関する情報。また、使用されたオプティマイザ、損失関数、チューニングされたハイパーパラメータなど、トレーニング手法の詳細も含まれます。
パフォーマンス測定基準：精度、正確度、再現率、F1スコアなど、さまざまな測定基準におけるモデルのパフォーマンスに関する情報。また、データの異なるサブセットでのモデルのパフォーマンスに関する情報も含まれます。
潜在的なバイアスと限界：不均衡なトレーニングデータ、オーバーフィッティング、モデルの予測におけるバイアスなど、モデルの潜在的なバイアスや制限をリストアップします。また、新しいデータへの汎化能力や特定のユースケースへの適合性など、モデルの限界に関する情報も含まれます。
責任あるAIへの配慮：プライバシーに関する懸念、公平性、透明性、またはモデルの使用による潜在的な社会的影響など、モデルに関連する倫理的または責任あるAIの考慮事項。また、モデルのさらなるテスト、検証、モニタリングに関する推奨事項が含まれる場合もあります。

モデルカードに含まれる正確な機能は、モデルのコンテキストと使用目的によって異なる可能性がありますが、その目的は、機械学習モデルの作成と展開に公開性と説明責任を与えることです。

モデルカードのレビュー
リスクカードがある場合はそれを確認
サードパーティを通じて使用されるモデルを含む、あらゆる導入モデルのモデルカードを追跡し、維持するプロセスを確立します。

### RAG：大規模言語モデルの最適化

ファインチューニングは、事前に訓練されたモデルを最適化するための伝統的な手法で、既存のモデルを新しいドメイン固有のデータで再学習させ、タスクやアプリケーションのパフォーマンスに合わせて修正します。ファインチューニングにはコストがかかりますが、パフォーマンス向上には不可欠です。

検索補強型生成（RAG）は、最新の利用可能な知識ソースから適切なデータを検索することにより、大規模な言語モデルの能力を最適化し補強する、より効果的な方法として発展してきました。RAGは特定のドメイン用にカスタマイズすることができ、ドメイン固有の情報の検索を最適化し、特殊な分野のニュアンスに合わせて生成プロセスを調整します。RAGは、LLM最適化のための、より効率的で透明性の高い手法と考えられており、特にラベル付きデータの収集が限られていたり、高価であったりする問題に適しています。RAGの主な利点の一つは、検索段階で新しい情報を継続的に更新することができるため、継続的な学習をサポートすることです。

RAGの実装には、埋め込みモデルの展開から始まり、知識ライブラリの索引付け、クエリ処理のための最も関連性の高い文書の検索まで、いくつかの重要なステップが含まれます。関連するコンテキストの効率的な検索は、文書埋め込みモデルの保存とクエリに使用されるベクトルデータベースに基づいて行われます。

#### RAG 参照・リンク
検索拡張世代（RAG）とLLM：例
12 RAGの問題点と解決案

### AIレッドチーム

AIレッドチームとは、攻撃者が悪用できる脆弱性が存在しないことを確認するために、AIシステムを敵対的に攻撃するテストシミュレーションです。これは、バイデン政権を含む多くの規制機関やAI管理機関によって推奨されています。レッドチーミングだけでは、AIシステムに関連するすべての実害を検証する包括的な解決策とはならず、アルゴリズムによる影響評価や外部監査など、他の形式のテスト、評価、検証、妥当性確認と組み合わせる必要があります。

AIモデルとアプリケーションの標準的なプラクティスとして、レッドチームテストを導入します。
## 第四章　リソース・リンク

#### OWASP 大規模言語モデル・アプリケーション リスク トップ10

LLM01: プロンプト・インジェクション
巧妙な入力によって大規模な言語モデル（LLM）を操作し、LLMが意図しない動作を引き起こします。直接注入はシステムのプロンプトを上書きし、間接注入は外部ソースからの入力を操作するものです。

LLM02: 安全が確認されていない出力ハンドリング
この脆弱性は、LLM の出力が精査されずに受け入れられ、バックエンドシステムを露出させるという場合に起きることです。悪用されると、XSS、CSRF、SSRF、特権の昇格、リモート・コードの実行といった深刻な結果につながる可能性があります。

LLM03: 訓練データの汚染
LLM の訓練データが改ざんされ、セキュリティ、有効性、倫理的行動を損なう脆弱性やバイアスなどが入った状態です。情報源としては、Common Crawl、WebText、OpenWebText、書籍などがあります。

LLM04: モデルのDoS
攻撃者はLLM上でリソースを大量に消費する操作を引き起こすことで、サービスの低下や高コストをもたらします。LLMはリソースを大量に消費し、ユーザーの入力が予測できないため、脆弱性は拡大します。

LLM05: サプライチェーンの脆弱性
LLMアプリケーションのライフサイクルは、脆弱なコンポーネントやサービスによって侵害される可能性があり、セキュリティ攻撃につながります。サードパーティのデータセット、事前に訓練されたモデル、およびプラグインを使用することで、脆弱性が増える可能性があります。

LLM06: 機微情報の漏えい
LLMは、その応答の中で不注意に機密データを暴露する可能性があり、不正なデータアクセス、プライバシー侵害、セキュリティ侵害につながります。これを軽減するためには、データのサニタイズと厳格なユーザー・ポリシーを導入することが極めて重要です。

LLM07: 安全が確認されていないプラグイン設計
LLMプラグインにおいて、入力の安全性が確認されておらず、あるいはアクセスコントロールが不十分である場合、このようなアプリケーションにおけるコントロールの欠如は、悪用が容易であり、リモート・コード実行のような結果をもたらす可能性があります。

LLM08: 過剰な代理行為
LLMベースのシステムは、意図しない結果を招く動作をすることがあります。この問題は、LLMベースのシステムに与えられた過剰な機能、権限、または自律性に起因します。

LLM09: 過度の信頼
十分監督されていないLLMに過度に依存したシステムや人々は、LLMが生成したコンテンツが不正確または不適切なものである場合、誤った情報、誤ったコミュニケーション、法的問題、セキュリティの脆弱性に直面する可能性があります。

LLM10: モデルの盗難
これには、独自のLLMモデルへの不正アクセス、コピー、または流出が含まれます。その影響には、経済的損失、競争上の優位性の低下、機密情報へのアクセスの可能性などが含まれます。

##### 図 4.1　OWASP 大規模言語モデル・アプリケーション リスク トップ10
#### OWASP 大規模言語モデル・アプリケーション リスクの所在箇所
![OWASP Top10 for LLM Visual](images/GOV1_Fig_4_2.jpg)
##### 図 4.2　OWASP 大規模言語モデル・アプリケーション リスクの所在箇所

OWASP リソース LLM ソリューションを使用すると、組織の攻撃対象領域が拡大し、新たな課題が発生するため、特別な戦術や防御が必要になります。また、既知の問題と類似した問題も発生し、すでに確立されたサイバーセキュリティの手順と緩和策が存在します。LLMサイバーセキュリティを組織の確立されたサイバーセキュリティ管理、プロセス、手順と統合することで、組織は脅威に対する脆弱性を減らすことができます。これらがどのように統合されるかは、OWASP Integration Standards（OWASP統合基準）をご覧ください。

#### OWASP リソース・リンク
OWASP SAMM
#### 説明
ソフトウェア保証成熟度モデル
#### お勧めする理由と場所、使用方法
効果的で組織の安全な開発ライフサイクルを分析し、改善するための測定可能な方法。SAMM は、ソフトウェアライフサイクル全体をサポートします。  SAMM は相互作用的でリスク駆動型であるため、組織はセキュアなソフトウエア開発におけるギャップを特定し、優先順位を付けることができます。

#### OWASP リソース・リンク
OWASP AIセキュリティとプライバシーガイド
#### 説明
AIセキュリティに関する交流、標準の整合性の促進、コラボレーションの推進を目的として、世界中を結ぶことを目的としているOWASPプロジェクトです。
#### お勧めする理由と場所、使用方法
OWASP AIセキュリティとプライバシー　ガイドは、AIのセキュリティとプライバシーに関する最も重要な考慮事項の包括的なリストです。開発者、セキュリティ研究者、セキュリティ・コンサルタントがAIシステムのセキュリティとプライバシーを検証するための包括的なリソースとなることを目的としています。

#### OWASP リソース・リンク
OWASP AI Exchange
#### 説明
OWASP AI ExchangeはOWASP AIセキュリティ・プライバシーガイドのための摂取方法です。
#### お勧めする理由と場所、使用方法
AIエクスチェンジは OWASP が OWASP AIセキュリティ・プライバシーガイドの方向性を推進するために使用する摂取方法。

#### OWASP リソース・リンク
OWASP 機械学習セキュリティ トップ10
#### 説明
機械学習システムのセキュリティ問題トップ10。
#### お勧めする理由と場所、使用方法
OWASP Machine Learning Security Top 10は、機械学習システムの最も重要なセキュリティ問題を収集し、セキュリティ専門家とデータサイエンティストの両方が理解しやすい形式で提示するコミュニティ主導の取り組みです。このプロジェクトは、ML Top 10 を含み、安全でプライバシーを保護する AI システムの設計、作成、テスト、調達に関する明確で実行可能な洞察を提供する、生きた作業文書です。これは、AIのグローバルな規制とプライバシー情報のための最高のOWASPリソースです。

#### OWASP リソース・リンク
OpenCRE
#### 説明
OpenCRE (共通要件列挙)は、セキュリティ標準とガイドラインを1つの概要にまとめるためのインタラクティブなコンテンツリンクプラットフォームです。
#### お勧めする理由と場所、使用方法
標準、規格を検索するため。規格名またはコントロールタイプで検索できます。

#### OWASP リソース・リンク
OWASP脅威モデリング
#### 説明
構造化された正式なアプリケーションの脅威モデリングプロセス
#### お勧めする理由と場所、使用方法
脅威モデリングについて知ることができます。脅威モデリングとは、アプリケーションのセキュリティに関する情報を体系的にまとめています。

#### OWASP リソース・リンク
OWASP CycloneDX
#### 説明
OWASP CycloneDXは、サイバーリスク低減のための高度なサプライチェーン機能を提供するフルスタックの部品表（BOM）規格です。
#### お勧めする理由と場所、使用方法
最新のソフトウェアはサードパーティやオープンソースのコンポーネントを使用しています。これらのコンポーネントは、複雑かつ独自の方法で接着され、目的の機能を実現するために元のコードと統合されます。SBOM は、組織がリスクを特定し、透明性を高め、迅速な影響分析を可能にする、すべてのコンポーネントの正確なインベントリを提供します。EO 14028 は、連邦システムの SBOM に関する最低要件を規定。

#### OWASP リソース・リンク
OWASP ソフトウェアコンポーネント検証規格（SCVS）
#### 説明
コミュニティ主導アクティビティ、コントロール、ベストプラクティスを特定するためのフレームワークを確立する取り組みは、ソフトウェアサプライチェーンにおけるリスクの特定と削減に役立ちます。
#### お勧めする理由と場所、使用方法
SCVSを使って共通のソフトウェアサプライチェーンのリスクを低減し、成熟したソフトウェアサプライチェーンへの警戒のベースラインと道筋を特定することができる活動、管理、ベストプラクティスのセットです。

#### OWASP リソース・リンク
OWASP API セキュリティ プロジェクト
#### 説明
アプリケーション・プログラミング・インターフェース（API）のセキュリティに関するユニークな脆弱性とセキュリティリスクを理解し、軽減するための戦略とソリューションについて、説明しています。
#### お勧めする理由と場所、使用方法
APIは基盤となる要素ユーザと組織を保護するためには、アプリケーションを接続し、誤設定や脆弱性を緩和することが必須です。ビルド環境と本番環境のセキュリティテストとレッドチームに使用してください。

#### OWASP リソース・リンク
OWASP アプリケーション セキュリティ 検証基準 ASVS
#### 説明
アプリケーション・セキュリティ検証標準（ASVS）プロジェクトは、ウェブアプリケーションの技術的なセキュリティ管理をテストするための基礎を提供します。   プロジェクトは、ウェブアプリケーションの技術的なセキュリティ制御をテストするための基礎を提供し、また、開発者に安全な開発のための要求事項のリストを提供します。
#### お勧めする理由と場所、使用方法
ウェブアプリケーション用クックブックセキュリティ要件、セキュリティテスト、メトリクスセキュリティユーザストーリーとセキュリティユースケースのリリーステストを確立するために使用します。

#### OWASP リソース・リンク
OWASP 脅威およびセーフガードマトリックス (TaSM)
#### 説明
ビジネスを守り、可能にするために、行動重視の視点を提供します。
#### お勧めする理由と場所、使用方法
このマトリックスにより、企業は以下のことが可能になります。主要な脅威をNISTサイバーセキュリティフレームワークの機能（特定、保護、検知、対応、回復）に重ね合わせ、強固なセキュリティ計画を構築します。ダッシュボードとして使用して、組織全体のセキュリティを追跡し、報告します。

#### OWASP リソース・リンク
欠陥 道場
#### 説明
テンプレート化、レポート作成、数値化、基準となるセルフサービスツールにより、テストプロセスを合理化する、オープンソースの脆弱性管理ツールです。
#### お勧めする理由と場所、使用方法
「欠陥 道場」を使えば、テンプレートを使い、一般的な脆弱性を検出し、レポート生成、数値化し、脆弱性のログ作成時間を短縮できます。

### MITRE のリソース

LLM の脅威が頻発するようになったことで、組織の攻撃対象領域を防御するためのレジリエン ス第一のアプローチの価値が強調されています。既存の TTPS は、LLM における新たな攻撃対象領域と機能と組み合わされています。MITRE は、実世界での観察に基づき、敵対者の戦術と手順を調整するための、確立され広く受け入れられているメカニズムを維持しています。

組織の LLM セキュリティ戦略を MITRE ATT&CK と MITRE ATLAS に調整しマッピングすることで、LLM セキュリティが API セキュリティ標準などの現在のプロセスでカバーされている部分や、セキュリティホールが存在する部分を特定することができます。

MITRE ATT&CK（Adversarial Tactics, Techniques, and Common Knowledge）は、MITRE Corporationによって作成されたフレームワーク、データマトリックス集、および評価ツールです。これは世界中で使用されている知識リポジトリです。MITRE ATT&CKマトリックスには、敵対者が特定の目標を達成するために使用する戦略のコレクションが含まれています。ATT&CK マトリックスでは、これらの目的は戦術として分類されています。目的は攻撃順に概説されており、偵察から始まり、最終的な目標である殲滅または影響へと進んでいきます。

MITRE ATLASは「Adversarial Threat Landscape for Artificial Intelligence Systems」の略で、悪質な行為者による機械学習（ML）システムへの攻撃の実例に基づいた知識ベースです。ATLASはMITREのATT&CKアーキテクチャに基づいており、その戦術と手順はATT&CKに見られるものを補完しています。

#### MITRE リソース・リンク
MITRE ATT&CK
#### 説明
実際の観察に基づく敵の戦術とテクニックの知識ベース
#### お勧めする理由と場所、使用方法
具体的な脅威モデルと方法論の開発の基礎として使用します。組織内の既存の管理策を敵の戦術や技法にマップし、ギャップやテストすべき領域を特定します。

#### MITRE リソース・リンク
MITRE AT&CK 作業台
#### 説明
ローカルな知識ベースを使って、ATT&CKの作成または拡張する際に使う作業台です。
#### お勧めする理由と場所、使用方法
ATT&CK 知識ベースをコピーし、新しい、または更新されたテクニック、戦術、ミティゲーション・グループ、および組織に特化したソフトウェアで拡張することができます。

#### MITRE リソース・リンク
MITRE ATLAS
#### 説明
MITRE　ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) is a globally accessible, living knowledge base of adversary tactics and techniques against Al-enabled systems based on real-world attack observations and realistic demonstrations from Al red teams and security groups.
#### お勧めする理由と場所、使用方法
Use it to map known ML vulnerabilities and map checks and controls for proposed projects or existing systems.

#### MITRE リソース・リンク
MITRE ATT&CK Powered Suit
#### 説明
ATT&CKパワードスーツはは、MITRE ATT&CKナレッジベースをあなたの手元に置くブラウザ拡張機能です。
#### お勧めする理由と場所、使用方法
ATT&CKパワードスーツはは、MITRE ATT&CK ナレッジベースをあなたの手元に置くブラウザ拡張機能です

#### MITRE リソース・リンク
The Threat Report ATT&CK Mapper (TRAM)
#### 説明
CTI レポートのTTP Identificationを自動化する
#### お勧めする理由と場所、使用方法
CTIレポートに見られるTTPをMITRE ATT&CKに対応させるのは難しく、エラーが発生しやすく、時間がかかります。TRAMはLLMを使用し、最も一般的な50の技術についてこのプロセスを自動化します。Juypterノートブックをサポート

#### MITRE リソース・リンク
Attack Flow v2.1.0
#### 説明
アタックフローはサイバー敵対者が、様々な攻撃技術をどのように組み合わせ、目的を達成するのかを記述するための言語。
#### お勧めする理由と場所、使用方法
攻撃者がどのようなテクニックを使うかを理解することで、防御者や指導者は敵の動きを理解し、自らの防御態勢を向上させることができます。

#### MITRE リソース・リンク
MITRE カルデラ
#### 説明
敵のエミュレーションを自動化し、手動レッドチームを支援し、インシデントレスポンスを自動化するように設計された、サイバーセキュリティ・プラットフォームです。
#### お勧めする理由と場所、使用方法（プラグインへのリンク付）
Caldera 用のプラグイン 利用可能です。フレームワークのコア機能を拡張し、エージェント、レポート、TTP のコレクションなどの追加機能を提供します。

#### MITRE リソース・リンク
CALDERA ラグイン: アーセナル
#### 説明
AI対応システムの敵対的エミュレーションのために開発されたプラグインです。
#### お勧めする理由と場所、使用方法
このプラグインは、CALDERA とインターフェースするため、MITRE ATLAS に定義された TTP を提供します。

#### MITRE リソース・リンク
アトミック・レッド・チーム
#### 説明
MITRE ATT&CKフレームワークで使うテストのライブラリ
#### お勧めする理由と場所、使用方法
コントロールの検証とテストのために使用することができます。セキュリティチームは、Atomic Red Team を使用することで、自分たちの環境を素早く、ポータブルに、再現性よくテストすることができます。アトミックテストはコマンドラインから直接実行できます。インストレーションする必要はありません。

#### MITRE リソース・リンク
MITRE CTI ブループリント
#### 説明
サイバー脅威情報のレポートを自動化する
#### お勧めする理由と場所、使用方法
CTIブループリントは、サイバー脅威インテリジェンス（CTI）アナリストが、高品質で実用的なレポートをより一貫して効率的に作成するのを支援します。


### AI脆弱性リポジトリ

#### 名称
AI インシデント・データベース
#### 説明
過去に失敗したAIアプリケーションの事例を蓄積しています。大学の研究グループによって維持され、クラウドソーシングされています。

#### 名称
OECD AI インシデント・モニター (AIM)
#### 説明
AIに関連する課題を理解するためのわかりやすい出発点を提供します。

### AIモデルの脆弱性を追跡する大手企業3社

Huntrバグバウンティ　：　プロテクトAI
AI/ML向けバグ報奨金プラットフォーム

AI脆弱性データベース(AVID) ：　ガラク
モデルの脆弱性データベース

AIリスクデータベース　：　ロバスト・インテリジェンス
モデルの脆弱性データベース

### AI調達ガイダンス

#### 名称
世界経済フォーラム：AI責任の導入：民間セクターによるAIソリューション調達のためのガイドライン：インサイトレポート 2023年6月
#### 説明
人工知能システムの調達のための、標準ベンチマークと評価基準は、まだ開発初期段階にあります。この調達ガイドラインは、エンド・ツー・エンドの調達プロセスにおける考慮事項の基準を組織に提供します。このガイダンスを使うと、既存のサードパーティリスクサプライヤーとベンダー調達プロセスを強化することができます。
## チーム

OWASP 大規模言語モデル アプリケーション リスク トップ10　サイバーセキュリティとガバナンス チェックリストの作成に貢献された皆さんに感謝いたします。

### チェックリストの作成

Sandy Dunn
Heather Linn
John Sotiropoulos
Steve Wilson
Fabrizio Cilli
Aubrey King
Bob Simonoff
David Rowe
Ken Huang
Emmanual Guilherme Junior
Andrea Succi
Jason Ross
Talesh Seeparsan
Anthony Glynn
Julie Tao
Cédric Lallier
Tetsuo Seto
Ads Dawson

### 日本語版の作成

瀬戸　哲夫（Tetsuo Seto）
岡田さん（Okada）
