## Festlegen der LLM-Strategie

Die rasche Expansion von Anwendungen auf Basis großer Sprachmodelle (LLM) hat die Aufmerksamkeit und Untersuchung aller KI/ML-Systeme, die in Geschäftsprozessen verwendet werden, verstärkt. Dies umfasst sowohl Generative KI als auch lang etablierte Vorhersage-KI/ML-Systeme. Diese erhöhte Fokussierung legt potenzielle Risiken offen, wie etwa Angreifer, die Systeme ins Visier nehmen und zuvor übersehen wurden, und Governance- oder rechtliche Herausforderungen, die möglicherweise in Bezug auf rechtliche, Datenschutz-, Haftungs- oder Garantiefragen bislang ignoriert wurden. Für jede Organisation, die KI/ML-Systeme in ihren Abläufen einsetzt, ist es entscheidend, umfassende Richtlinien, Governance, Sicherheitsvorgehen, Datenschutzmaßnahmen und Verantwortungsstandards zu bewerten und zu etablieren, um sicherzustellen, dass diese Technologien bezüglich Sicherheit und Ethik zu den Geschäftsprozessen passen.

Angreifer oder Gegner stellen die unmittelbarste und schädlichste Bedrohung für Unternehmen, Personen und Regierungsbehörden dar. Ihre Ziele, die von finanziellem Gewinn bis hin zu Spionage reichen, treiben sie dazu, kritische Informationen zu stehlen, den Betrieb zu stören und das Vertrauen zu schädigen. Dazu erwerben sie durch KI und maschinelles Lernen die Möglichkeit, mit mehr Geschwindigkeit und Raffinesse den Verteidigern voraus zu sein.

Die dringendste Nicht-Gegner-LLM-Bedrohung für viele Organisationen stammt von "Shadow AI": Mitarbeiter, die nicht genehmigte Online-KI-Tools, unsichere Browser-Plugins und Drittanbieteranwendungen verwenden, die LLM-Funktionen durch Updates oder Upgrades einführen und damit standardisierte Softwaregenehmigungsprozesse umgehen.

>||center|16|16 Bereitstellung

>cornflower|white|left|14|18 Schritt 1: Resilienz-Strategie zuerst

    >fidlightblue|black|justified|12|16 ▶ Identifikation unmittelbarer Bedrohungen durch Bedrohungsmodellierung von Missbrauchsfällen
    >fidlightblue|black|justified|12|16 ▶ Überprüfen Sie interne oder externe Ausnutzungsmöglichkeiten in den Bedrohungsmodellszenarien und verifizieren Sie Sicherheitsmaßnahmen
    >fidlightblue|black|justified|12|16 ▶ Scannen und überwachen Sie Ihr Environment auf Vorkommen von Schadsoftware

>dodgerblue|white|left|14|18 Schritt 2: Bestehende Richtlinien aktualisieren

    >fidlightblue|black|justified|12|16 ▶ Verträge, Geheimhaltungsvereinbarungen, Governance und Sicherheit prüfen, um bei Bedarf Nutzung und Bedrohung durch LLMs oder GenAI zu integrieren

>fidblue|white|left|14|18 Schritt 3: Schulung / Bildung

    >fidlightblue|black|justified|12|16 ▶ Aktualisieren Sie das Sicherheitsbewusstseinstraining, Entwickler-, rechtliche oder andere Schulungen, um die Nutzung oder Bedrohung durch LLMs oder GenAI einzubeziehen

>darkblue|white|left|14|18 Schritt 4: Führungskräfte einbeziehen

    >fidlightblue|black|justified|12|16 ▶ Arbeiten Sie mit Führungskräften und anderen Stakeholdern zusammen, um LLM- oder GenAI-Lösungsstrategien zu identifizieren
    >fidlightblue|black|justified|12|16 ▶ Implementieren Sie eine Risikomanagementstrategie

>cornflower|white|left|14|18 Schritt 5: Aktualisieren Sie Ihr Programm zur Risikomanagement bei Drittanbietern

    >fidlightblue|black|justified|12|16 ▶ Lösungen von Drittanbietern und Anbietern für KI benötigen erweiterte Fragebögen und Überprüfungen

>dodgerblue|white|left|14|18 Schritt 6: Eine Bereitstellungsstrategie wählen

##### Abbildung 2.1 Optionen für Bereitstellungsstrategie
##### Quelle: sdunn


### Bereitstellungsstrategie

Die Bandbreite reicht von der Nutzung öffentlicher Endverbraucheranwendungen bis hin zum Training proprietärer Modelle auf eigenen Daten. Faktoren wie Sensibilität der Anwendungsfälle, benötigte Fähigkeiten und verfügbare Ressourcen helfen dabei, das passende Gleichgewicht zwischen Komfort und Kontrolle zu bestimmen. Das Verständnis dieser fünf Modelltypen bietet dabei einen Rahmen für die Bewertung der Optionen.

>||center|16|16 Bereitstellungstypen

>cornflower|white|left|14|18 Typ 1: Direkter Zugriff

    >fidlightblue|black|left|12|16 ▶ Große Sprachmodelle über eine Schnittstelle nutzen
    >fidlightblue|black|left|12|16 ▶ Risiko durch Unternehmensrichtlinien und Mitarbeitertraining mindern
    >fidlightblue|black|left|12|16 ▶ Vorteile: Flexibles und schnelles Experimentieren
    >fidlightblue|black|left|12|16 ▶ Beispiele: Perplexity, ChatGPT, big-AGI

>dodgerblue|white|left|14|18 Typ 2: Zugriff über Model API

    >fidlightblue|black|left|12|16 ▶ Große Sprachmodelle direkt von Anbietern über deren API nutzen
    >fidlightblue|black|left|12|16 ▶ Risiko mit Unternehmensrichtlinien und Mitarbeitertraining mindern
    >fidlightblue|black|left|12|16 ▶ Vorteile: Schnelles Experimentieren mit etwas zentraler Kontrolle über die API
    >fidlightblue|black|left|12|16 ▶ Beispiele: Claude, ChatGPT, Gemini

>fidblue|white|left|14|18 Typ 3: Lizenzmodell

    >fidlightblue|black|left|12|16 ▶ Ein lizenziertes großes Sprachmodell auf den eigenen Systemen betrieben
    >fidlightblue|black|left|12|16 ▶ Risiko durch Verwaltung durch Eigenbetrieb mindern. Risiko durch Unternehmensrichtlinien und Mitarbeitertraining reduzieren
    >fidlightblue|black|left|12|16 ▶ Vorteile: mehr Kontrolle und Integration durch internen Tools und Workflows
    >fidlightblue|black|left|12|16 ▶ Beispiele: Microsoft Enterprise CoPilot, Amazon Codewhisperer, SalesForce Einstein GPT

>darkblue|white|left|14|18 Typ 4: Vortrainiertes Modell

    >fidlightblue|black|left|12|16 ▶ Ein allgemeines Basismodell nutzen, dann durch Finetuning mit Unternehmens- oder benutzerdefinierten Daten anpassen
    >fidlightblue|black|left|12|16 ▶ Risiko mit erhöhter Transparenz mindern, Leistung verbessern, Halluzinationen reduzieren. Risiko mit Unternehmensrichtlinien und Mitarbeitertraining reduzieren
    >fidlightblue|black|left|12|16 ▶ Vorteile: Verbesserte Leistung und reduzierte Halluzinationen
    >fidlightblue|black|left|12|16 ▶ Beispiele: QwenLM/Qwen 1.5, DBRX, Starling 7B

>cornflower|white|left|14|18 Typ 5: Bewährtes Modell finetunen

    >fidlightblue|black|left|12|16 ▶ Bewährte (spezialisierte) Modelle nutzen und weiter mit eigenen Daten feinabstimmen, um sie an Ihr Unternehmen anzupassen
    >fidlightblue|black|left|12|16 ▶ Risiko mit erhöhter Transparenz mindern, Leistung verbessern, Halluzinationen reduzieren. Risiko mit Unternehmensrichtlinien und Mitarbeitertraining reduzieren
    >fidlightblue|black|left|12|16 ▶ Vorteile: Ermöglicht Anpassung über vortrainierte Modelle hinaus
    >fidlightblue|black|left|12|16 ▶ Beispiele: Google MedPalm, Amazon Bedrock, Llama2, LegalAI

>dodgerblue|white|left|14|18 Typ 6: Individuelle Modelle

    >fidlightblue|black|left|12|16 ▶ Eine maßgeschneiderte KI/ML-Modellarchitektur für spezifische Unternehmensanwendungsfälle bauen
    >fidlightblue|black|left|12|16 ▶ Risiko mit vollständiger Sichtbarkeit und Kontrolle mindern. Risiko mit Unternehmensrichtlinien, Entwickler- und Mitarbeitertraining reduzieren
    >fidlightblue|black|left|12|16 ▶ Vorteile: Erfordert große Investitionen, maximiert aber die Anpassbarkeit

##### Abbildung 2.2 Optionen für Bereitstellungstypen
##### Quelle: sdunn