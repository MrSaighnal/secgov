## Übersicht

Jeder Internetnutzer und jedes Unternehmen muss sich auf die kommende Welle leistungsfähiger generativer künstlicher Intelligenz (GenAI) Anwendungen vorbereiten. GenAI verspricht enorme Innovationen, Effizienz und kommerzielle Vorteile in einer Vielzahl von Branchen. Doch wie jede leistungsstarke Technologie in einem frühen Entwicklungsstadium birgt sie ihre eigenen offensichtlichen und unerwarteten Herausforderungen.

Künstliche Intelligenz hat in den letzten 50 Jahren große Fortschritte gemacht und eine Vielzahl von Unternehmensprozessen unauffällig unterstützt, bis das öffentliche Erscheinen von ChatGPT die Entwicklung und Nutzung von Large Language Models (LLMs) sowohl bei Einzelpersonen als auch bei Unternehmen vorangetrieben hat. Anfangs waren diese Technologien auf akademische Studien oder die Ausführung bestimmter, aber wichtiger Aktivitäten innerhalb von Unternehmen beschränkt und nur für wenige Auserwählte sichtbar. Die jüngsten Fortschritte in der Verfügbarkeit von Daten, Computerleistung, GenAI-Fähigkeiten und die Veröffentlichung von Tools wie Llama 2, ElevenLabs und Midjourney haben AI jedoch von einer Nische zu einer allgemein weitverbreiteten Akzeptanz gebracht. Diese Verbesserungen haben nicht nur die GenAI-Technologien zugänglicher gemacht, sondern auch den kritischen Bedarf für Unternehmen hervorgehoben, solide Strategien für die Integration und Nutzung von AI in ihren Geschäftstätigkeiten zu entwickeln, was einen großen Schritt nach vorn in der Art und Weise darstellt, wie wir Technologie nutzen.

Künstliche Intelligenz ist ein breiter Begriff, der alle Bereiche der Informatik umfasst, die es Maschinen ermöglichen, Aufgaben auszuführen, die normalerweise menschliche Intelligenz erfordern würden. Maschinelles Lernen und generative AI sind zwei Unterkategorien von KI.
Maschinelles Lernen ist eine Unterkategorie von KI, die sich auf die Erstellung von Algorithmen konzentriert, die aus Daten lernen können. Maschinelle Lernalgorithmen werden mit einem Datensatz trainiert und können dann diese Daten verwenden, um Vorhersagen oder Entscheidungen über neue Daten zu treffen.
Generative AI ist eine Art von maschinellem Lernen, die sich auf die Erstellung neuer Daten konzentriert. Oftmals stützt sich GenAI auf die Verwendung von Large Language Models, um die erforderlichen Aufgaben zur Erstellung der neuen Daten durchzuführen.
Ein Large Language Model (LLM) ist eine Art von KI-Modell, das menschenähnlichen Text verarbeitet und generiert. Im Kontext der künstlichen Intelligenz bezieht sich ein "Modell" auf ein System, das trainiert wurde, um Vorhersagen basierend auf Eingabedaten zu treffen. LLMs werden speziell auf großen Datensätzen natürlicher Sprache trainiert, daher der Name Large Language Models.

Organisationen betreten Neuland bei der Sicherung und Überwachung von GenAI-Lösungen. Der schnelle Fortschritt von GenAI eröffnet auch Türen für Gegner, ihre Angriffsstrategien zu verbessern und stellt eine doppelte Herausforderung aus Verteidigung und Bedrohungseskalation dar.

Unternehmen verwenden künstliche Intelligenz in vielen Bereichen, einschließlich HR, für  Recruiting, E-Mail-Spam-Filterung, SIEM für Verhaltensanalysen und Managed Detection and Response-Anwendungen. Der Schwerpunkt dieses Dokuments liegt jedoch auf Large Language Model-Anwendungen und ihrer Funktion bei der Erstellung generierter Inhalte.

### Verantwortungsvolle und vertrauenswürdige künstliche Intelligenz

Da Herausforderungen und Vorteile der künstlichen Intelligenz entstehen – und Vorschriften und Gesetze verabschiedet werden – entwickeln sich die Prinzipien und Säulen der verantwortungsvollen und vertrauenswürdigen KI-Nutzung von idealistischen Objekten und Bedenken kommend zu etablierten Standards.

Die OWASP AI Exchange Working Group verfolgt diese Veränderungen und geht die breiteren und herausfordernderen Überlegungen für alle Aspekte der künstlichen Intelligenz an.

Vertrauenswürdige künstliche Intelligenz

ZUVERLÄSSIG
Robust
Verantwortlich
Überwacht
Transparent
Erklärbar

RESILIENT
Sicher
Geschützt
Privat
Effektiv

VERANTWORTUNGSVOLL
Fair
Ethisch
Inklusiv
Nachhaltig
Zweckmäßig

##### Abbildung 1.1 Säulen der vertrauenswürdigen künstlichen Intelligenz
##### erstellt vom Montreal Ethics Institute Beispiel

### Für wen ist dies?

Die OWASP Top 10 für LLM-Anwendungen-Cybersecurity-und-Governance-Checkliste ist für Führungskräfte aus der Exekutive, Technik, Cybersicherheit, Datenschutz, Compliance und Recht, DevSecOps, MLSecOps und Cybersicherheitsteams und Verteidiger gedacht. Sie richtet sich an Personen, die bestrebt sind, in der schnelllebigen KI-Welt einen Schritt voraus zu sein, mit dem Ziel, KI nicht nur für den Unternehmenserfolg zu nutzen, sondern auch gegen die Risiken überstürzter oder unsicherer KI-Implementierungen zu schützen. Diese Führungskräfte und Teams müssen Taktiken entwickeln, um Chancen zu ergreifen, Herausforderungen zu bekämpfen und Risiken zu mindern.

Diese Checkliste soll diesen Technologie- und Geschäftsführern dabei helfen, schnell die Risiken und Vorteile der Nutzung von LLM zu verstehen und sich auf die Entwicklung einer umfassenden Liste kritischer Bereiche und Aufgaben zu konzentrieren, die benötigt werden, um die Organisation zu verteidigen und zu schützen, während sie eine Large Language Model-Strategie entwickeln.

Es ist die Hoffnung des OWASP Top 10 für das LLM-Anwendungen-Teams, dass diese Liste Organisationen helfen wird, ihre bestehenden Verteidigungstechniken zu verbessern und Techniken zu entwickeln, um die neuen Bedrohungen anzugehen, die aus der Nutzung dieser aufregenden Technologie resultieren.

### Warum eine Checkliste?

Checklisten, die zur Formulierung von Strategien verwendet werden, verbessern die Genauigkeit, definieren Ziele, bewahren die Einheitlichkeit und fördern zielgerichtetes, bedachtes Arbeiten, wodurch das Übersehen und Verpassen von Details reduziert werden. Die Nutzung einer Checkliste erhöht nicht nur das Vertrauen in eine sichere Einführung, sondern fördert auch zukünftige Organisationsinnovationen, indem sie eine einfache und effektive Möglichkeit für kontinuierliche Verbesserungen bietet.

### Nicht umfassend

Obwohl dieses Dokument Organisationen dabei unterstützen soll, eine anfängliche LLM-Strategie in einer sich schnell ändernden technischen, rechtlichen und regulatorischen Umgebung zu entwickeln, ist es nicht erschöpfend und deckt nicht jeden Anwendungsfall oder jede Verpflichtung ab. Die Nutzung dieses Dokuments soll Organisationen dazu anregen, Bewertungen und Praktiken über den Umfang der bereitgestellten Checkliste hinaus zu erweitern, wie es für ihren Anwendungsfall oder ihre Zuständigkeit erforderlich ist.

### Die Herausforderungen von Large Language Models

Large Language Models stehen vor mehreren ernsthaften und einzigartigen Problemen. Eines der wichtigsten ist, dass beim Arbeiten mit LLMs die Steuerungs- und Datenebenen nicht strikt isoliert oder trennbar sind. Eine weitere wichtige Herausforderung ist, dass LLMs von Natur aus nichtdeterministisch sind und bei Aufforderung oder Anfrage ein anderes Ergebnis liefern. LLMs verwenden semantische Suche anstelle von Schlüsselwortsuche. Der Kernunterschied zwischen den beiden ist, dass der Algorithmus des Modells die Begriffe in seiner Antwort priorisiert. Dies ist eine bedeutende Abweichung von der Art und Weise, wie Verbraucher bisher Technologie genutzt haben, und es hat Auswirkungen auf die Konsistenz und Zuverlässigkeit der Ergebnisse. Halluzinationen, die aus den Lücken und Trainingsfehlern in den Daten resultieren, auf denen das Modell trainiert wurde, sind das Ergebnis dieser Methode.

Es gibt Methoden, um die Zuverlässigkeit zu verbessern und die Angriffsfläche für Jailbreaking, Modelltäuschungen und Halluzinationen zu reduzieren, aber es gibt einen Kompromiss zwischen Einschränkungen und Nutzen sowohl in Kosten als auch in Funktionalität.

Die Nutzung und Anwendungen von LLM erhöhen die Angriffsfläche einer Organisation. Einige Risiken der Risiken von LLM-Applikationen sind einzigartig, aber viele sind bekannte Probleme, wie die bekannte Software Bill of Materials (SBoM), die Lieferkette, Data Loss Protection (DLP) und unautorisierter Zugriff. Es gibt auch erhöhte Risiken, die nicht direkt mit GenAI zusammenhängen, bei denen GenAI die Effizienz, Fähigkeit und Wirksamkeit von Angreifern, die Organisationen, Einzelpersonen und Regierungssysteme angreifen und bedrohen erhöht.

Gegner nutzen zunehmend LLM- und Generative AI-Tools, um traditionelle Methoden des Angriffs auf Organisationen, Einzelpersonen und Regierungssysteme zu verbessern und zu beschleunigen. LLM verbessert ihre Fähigkeit, Techniken zu verbessern, um mühelos neue Malware zu erstellen, die möglicherweise mit neuartigen Zero-Day-Schwachstellen eingebettet ist oder darauf ausgelegt ist, die Erkennung zu umgehen. Sie können auch ausgeklügelte, einzigartige oder maßgeschneiderte Phishing-Schemata generieren. Die Erstellung überzeugender Deepfakes, sei es Video oder Audio, vereinfacht Social-Engineering. Zusätzlich vereinfachen diese Tools Einbrüche und innovative Hacking-Fähigkeiten zu entwickeln. In Zukunft wird der „maßgeschneiderte“ und kombinierte Einsatz von KI-Technologie durch kriminelle Akteure spezifische Antworten und dedizierte Lösungen für die angemessene Verteidigung und Widerstandsfähigkeit der Organisationen erfordern.

Organisationen gehen auch ein Risiko ein,wenn sie die Fähigkeiten von LLMs NICHT zu nutzen, haben einen Wettbewerbsnachteil. Sie werden auf dem Markt von Kunden und Partnern als veraltet, unfähig zum Skalieren personalisierter Kommunikation wahrgenommen. Ihnen wird Innovationsstagnation, betriebliche Ineffizienz, ein höheres Risiko menschlicher Fehler in Prozessen und ineffiziente Zuweisung von Humanressourcen unterstellt.

Das Verständnis der verschiedenen Arten von Bedrohungen und deren Integration in die Geschäftsstrategie hilft dabei, sowohl die Vor- als auch die Nachteile der Nutzung von Large Language Models (LLMs) gegenüber deren Nichtnutzung abzuwägen und sicherzustellen, dass sie die Geschäftsziele beschleunigen und nicht behindern.

### LLM-Bedrohungskategorien

KI-Bedrohungskarte

Bedrohungen durch NICHT-Nutzung von KI-Modellen
Bedrohungen durch Nutzung von KI-Modellen
Bedrohungen für KI-Modelle
Bedrohungen durch KI-Modelle
KI-rechtliche & regulatorische Bedrohungen

##### Abbildung 1.2 Arten von KI-Bedrohungen
##### credit sdunn



### Schulung zu Sicherheit und Datenschutz in der künstlichen Intelligenz

Mitarbeiter in Organisationen profitieren von Schulungen, um künstliche Intelligenz, generative künstliche Intelligenz und die zukünftigen potenziellen Konsequenzen des Aufbaus, Kaufs oder der Nutzung von LLMs zu verstehen. Schulungen für zulässige Nutzung und Sicherheitsbewusstsein sollten sich sowohl an alle Mitarbeiter richten als auch spezialisierter für bestimmte Positionen wie Personalwesen, Rechtsabteilung, Entwickler, Datenteams und Sicherheitsteams sein.

Richtlinien für faire Nutzung und gesunde Interaktion sind Schlüsselaspekte, die, wenn sie von Anfang an integriert werden, ein Eckpfeiler für den Erfolg zukünftiger KI-Cybersicherheitsbewusstseinskampagnen sein werden. Dies wird den Benutzern das Wissen über die grundlegenden Regeln für die Interaktion sowie die Fähigkeit vermitteln, gutes von schlechtem oder unethischem Verhalten zu unterscheiden.

### Integration von LLM-Sicherheit und -Governance mit bestehenden, etablierten
### Praktiken und Kontrollen

Obwohl KI und generative KI eine neue Dimension zu Cybersicherheit, Resilienz, Datenschutz und der Erfüllung rechtlicher und regulatorischer Anforderungen hinzufügen, sind die lang existierenden bewährten Praktiken immer noch der beste Weg, um Probleme zu identifizieren, Schwachstellen zu finden, diese zu beheben und potenzielle Sicherheitsprobleme zu reduzieren.

● Stellen Sie sicher, dass das Management von KI-Systemen mit bestehenden organisatorischen Praktiken integriert ist.
● Stellen Sie sicher, dass KI/ML-Systeme bestehenden Datenschutz-, Governance- und Sicherheitspraktiken folgen und bei Bedarf spezifische KI-Datenschutz-, Governance- und Sicherheitspraktiken implementiert werden.

### Grundlegende Sicherheitsprinzipien

LLM-Fähigkeiten führen einen anderen Typ von Angriff und Angriffsfläche ein. LLMs sind anfällig für komplexe Geschäftslogikfehler, wie z. B. Prompt-Injektion, unsicheres Plugin-Design und Remote-Code-Ausführung. Existierende Best Practices sind der beste Weg, um diese Probleme zu lösen. Ein internes Produktsicherheitsteam, das sichere Softwareüberprüfung, Architektur, Data-Governance und Drittanbieterbewertungen versteht, muss überprüfen, wie stark die aktuellen Mechanismen sind, um Probleme zu finden, die durch LLM verschlimmert werden könnten, wie z. B. Stimmklonung, Personifizierung oder das Umgehen von Captchas.

Angesichts der jüngsten Fortschritte im maschinellen Lernen, NLP (Natural Language Processing), NLU (Natural Language Understanding), Deep Learning und in jüngerer Zeit LLMs (Large Language Models) und Generative AI wird empfohlen, Fachleute, die in diesen Bereichen versiert sind, bei Cybersicherheits- und Devops-Teams einzubeziehen. Ihre Expertise wird nicht nur bei der Einführung dieser Technologien, sondern auch bei der Entwicklung innovativer Analysen und Antworten auf neu entstehende Herausforderungen helfen.

### Risiko

Der Verweis auf Risiko verwendet die Definition as ISO 31000: Risiko = "Effekt der Unsicherheit auf Ziele." Zu den in der Checkliste enthaltenen LLM-Risiken gehört eine gezielte Liste von LLM-Risiken, die gegnerische, sicherheitstechnische, rechtliche, regulatorische, rufschädigende, finanzielle und wettbewerbliche Risiken ansprechen.

### Schwachstellen- und Minderungstaxonomie

Aktuelle Systeme zur Klassifizierung von Schwachstellen und zum Austausch von Bedrohungsinformationen, wie etwa OVAL, STIX, CVE und CWE, arbeiten noch an der  Fähigkeit, spezifische Schwachstellen und Bedrohungen in Bezug auf große Sprachmodelle (LLMs) und Prediction-Modelle zu überwachen und die Verteidiger zu alarmieren. Es wird erwartet, dass Organisationen sich auf diese etablierten und anerkannten Standards wie CVE für die Klassifizierung von Schwachstellen und STIX für den Austausch von Informationen über Cyber-Bedrohungen (CTI) stützen werden, wenn Schwachstellen oder Bedrohungen für KI/ML-Systeme und deren Lieferketten identifiziert werden.
## Determining LLM Strategy

The rapid expansion of Large Language Model (LLM) applications has heightened the attention and examination of all AI/ML systems used in business operations, encompassing both Generative AI and long-established Predictive AI/ML systems. This increased focus exposes potential risks, such as attackers targeting systems that were previously overlooked and governance or legal challenges that may have been disregarded in terms of legal, privacy, liability, or warranty issues. For any organization leveraging AI/ML systems in its operations, it's critical to assess and establish comprehensive policies, governance, security protocols, privacy measures, and accountability standards to ensure these technologies align with business processes securely and ethically.

Attackers, or adversaries, provide the most immediate and harmful threat to enterprises, people, and government agencies. Their goals, which range from financial gain to espionage, push them to steal critical information, disrupt operations, and damage confidence. Furthermore, their ability to harness new technologies such as AI and machine learning increases the speed and sophistication of attacks, making it difficult for defenses to stay ahead of attacks.

The most pressing non-adversary LLM threat for many organizations stem from "Shadow AI": employees using unapproved online AI tools, unsafe browser plugins, and third-party applications that introduce LLM features via updates or upgrades, circumventing standard software approval processes.

Deployment Steps

Step 1: Resilience First Strategy

▶ Identify immediate threats by Threat Modeling abuse cases
▶ Review internal or external exploitation cases for threat model scenarios and verify security controls
▶ Scan and monitor the environment for any instances of rogue applications

Ste 2: Update Existing Policy

▶ Review contracts, NDA, governance, and security to incorporate LLM or GenAI use or threat

Step 3: Training / Education

▶ Update security awareness training, developer, legal, or other training to include LLM or GenAI use or threat

Step 4: Engage with Leaders

▶ Work with executives, business leaders, and other stakeholders to identify an LLM or GenAI solutions strategy
▶ Implement mitigation / risk strategy

Step 5: Update Third Party Risk Management Program

▶ Third-party and vendor AI solutions will require augmented questionnaires and reviews

Step 6: Choose a Deployment Strategy

##### Figure 2.1  Options for Deployment Strategy
##### credit sdunn


### Deployment Strategy

The scopes range from leveraging public consumer applications to training proprietary models on private data. Factors like use case sensitivity, capabilities needed, and resources available help determine the right balance of convenience vs. control. However, understanding these five model types provides a framework for evaluating options.

Deployment Types

Type 1: Direct Access

▶ Use large language models from their interface
▶ Mitigate risk with corporate policy and employee training
▶ Pros: Flexible and rapid experimentation
▶ Examples: Perplexity, ChatGPT, big-AGI

Type 2: Access Through Model API

▶ Use large language models directly from vendors through their API
▶ Mitigate risk with corporate policy and employee training
▶ Pros: Enables quick experimentation with some central control through the API
▶ Examples: Claude, ChatGPT, Gemini

Type 3: Licensed Model

▶ Use a licensed large language model in enterprise tenant
▶ Mitigate risk by managing in own tenant.  Reduce risk with corporate policy and employee training
▶ Pros: Provides more control and integration with internal tools and workflow
▶ Examples: Microsoft Enterprise CoPilot, Amazon Codewhisperer, SalesForce Einstein GPT

Type 4: Pre-Trained Model

▶ Use a general foundation model, then customize by fine-tuning with enterprise or custom data
▶ Mitigate risk with increased visibility, enhance performance, reduce hallucinations.  Reduce risk with corporate policy and employee training
▶ Pros: enhance performance and reduce hallucinations
▶ Examples: QwenLM/Qwen 1.5, DBRX, Starling 7B

Type 5: Fine-Tune Proven Model

▶ Use a proen(specialized) models and fine-tune further with proprietary data to adapt them to your enterprise
▶ Mitigate risk with increased visibility, enhance performance, reduce hallucinations.  Reduce risk with corporate policy and employee training
▶ Pros: Enables customization beyond pre-trained
▶ Examples: Google MedPalm, Amazon Bedrock, Llama2, LegalAI

Type 6: Custom Models

▶ Build a tailored AI/ML Model architecture for enterprise specific use case
▶ Mitigate risk with complete visibility and control.  Reduce risk with corporate policy, developer, and employee training
▶ Pros: Requires large investment but maximizes customization

##### Figure 2.2 Options for Deployment Types
##### credit: sdunn
## Checklist

### Adversarial Risk

Adversarial Risk includes competitors and attackers.

Scrutinize how competitors are investing in artificial intelligence. Although there are risks in AI adoption, there are also business benefits that may impact future market positions.
Investigate the impact of current controls, such as password resets, which use voice recognition which may no longer provide the appropriate defensive security from new GenAI enhanced attacks.
Update the Incident Response Plan and playbooks for GenAI enhanced attacks and AIML specific incidents. 

### Threat Modeling

Threat modeling is highly recommended to identify threats and examine processes and security defenses. Threat modeling is a set of systematic, repeatable processes that enable making reasonable security decisions for applications, software, and systems. Threat modeling for GenAI accelerated attacks and before deploying LLMs is the most cost effective way to Identify and mitigate risks, protect data, protect privacy, and ensure a secure, compliant integration within the business.

How will attackers accelerate exploit attacks against the organization, employees, executives, or users? Organizations should anticipate "hyper-personalized" attacks at scale using Generative AI. LLM-assisted Spear Phishing attacks are now exponentially more effective, targeted, and weaponized for an attack.
How could GenAI be used for attacks on the business's customers or clients through spoofing or GenAI generated content?
Can the business detect and neutralize harmful or malicious inputs or queries to LLM solutions? 
Can the business safeguard connections with existing systems and databases with secure integrations at all LLM trust boundaries?
Does the business have insider threat mitigation to prevent misuse by authorized users?
Can the business prevent unauthorized access to proprietary models or data to protect Intellectual Property?
Can the business prevent the generation of harmful or inappropriate content with automated content filtering? 

### AI Asset Inventory

An AI asset inventory should apply to both internally developed and external or third-party solutions.

Catalog existing AI services, tools, and owners. Designate a tag in asset management for specific inventory.
Include AI components in the Software Bill of Material (SBOM), a comprehensive list of all the software components, dependencies, and metadata associated with applications.
Catalog AI data sources and the sensitivity of the data (protected, confidential, public)
Establish if pen testing or red teaming of deployed AI solutions is required to determine the current attack surface risk.
Create an AI solution onboarding process.
Ensure skilled IT admin staff is available either internally or externally, following SBoM requirements.

### AI Security and Privacy Training

Actively engage with employees to understand and address concerns with planned LLM initiatives.
 Establish a culture of open, and transparent communication on the organization's use of predictive or generative AI within the organization process, systems, employee management and support, and customer engagements and how its use is governed, managed, and risks addressed. 
Train all users on ethics, responsibility, and legal issues such as warranty, license, and copyright.
Update security awareness training to include GenAI related threats. Voice cloning and image cloning, as well as in anticipation of increased spear phishing attacks
Any adopted GenAI solutions should include training for both DevOps and cybersecurity for the deployment pipeline to ensure AI safety and security assurances.

### Establish Business Cases

Solid business cases are essential to determining the business value of any proposed AI solution, balancing risk and benefits, and evaluating and testing return on investment. There are an enormous number of potential use cases; a few examples are provided.

Enhance customer experience
Better operational efficiency
Better knowledge management
Enhanced innovation
Market Research and Competitor Analysis
Document creation, translation, summarization, and analysis 

### Governance

Corporate governance in LLM is needed to provide organizations with transparency and accountability. Identifying AI platform or process owners who are potentially familiar with the technology or the selected use cases for the business is not only advised but also necessary to ensure adequate reaction speed that prevents collateral damages to well established enterprise digital processes.

Establish the organization’s AI RACI chart (who is responsible, who is accountable, who should be consulted, and who should be informed)
Document and assign AI risk, risk assessments, and governance responsibility within the organization.
Establish data management policies, including technical enforcement, regarding data classification and usage limitations. Models should only leverage data classified for the minimum access level of any user of the system. For example, update the data protection policy to emphasize not to input protected or confidential data into nonbusiness-managed tools.
Create an AI Policy supported by established policy (e.g., standard of good conduct, data protection, software use)
Publish an acceptable use matrix for various generative AI tools for employees to use.
Document the sources and management of any data that the organization uses from the generative LLM models. 

### Legal

Many of the legal implications of AI are undefined and potentially very costly. An IT, security, and legal partnership is critical to identifying gaps and addressing obscure decisions.

Confirm product warranties are clear in the product development stream to assign who is responsible for product warranties with AI.
Review and update existing terms and conditions for any GenAI considerations.
Review AI EULA agreements. End-user license agreements for GenAI platforms are very different in how they handle user prompts, output rights and ownership, data privacy, compliance, liability, privacy, and limits on how output can be used.
Organizations EULA for customers, Modify end-user agreements to prevent the organization from incurring liabilities related to plagiarism, bias propagation, or intellectual property infringement through AI-generated content.
Review existing AI-assisted tools used for code development. A chatbot's ability to write code can threaten a company's ownership rights to its product if a chatbot is used to generate code for the product. For example, it could call into question the status and protection of the generated content and who holds the right to use the generated content.
Review any risks to intellectual property. Intellectual property generated by a chatbot could be in jeopardy if improperly obtained data was used during the generative process, which is subject to copyright, trademark, or patent protection. If AI products use infringing material, it creates a risk for the outputs of the AI, which may result in intellectual property infringement.
Review any contracts with indemnification provisions. Indemnification clauses try to put the responsibility for an event that leads to liability on the person who was more at fault for it or who had the best chance of stopping it. Establish guardrails to determine whether the provider of the AI or its user caused the event, giving rise to liability.
Review liability for potential injury and property damage caused by AI systems.
Review insurance coverage. Traditional (D&O) liability and commercial general liability insurance policies are likely insufficient to fully protect AI use.
Identify any copyright issues. Human authorship is required for copyright. An organization may also be liable for plagiarism, propagation of bias, or intellectual property infringement if LLM tools are misused.
Ensure agreements are in place for contractors and appropriate use of AI for any development or provided services.
Restrict or prohibit the use of generative AI tools for employees or contractors where enforceable rights may be an issue or where there are IP infringement concerns.
Assess and AI solutions used for employee management or hiring could result in disparate treatment claims or disparate impact claims.
Make sure the AI solutions do not collect or share sensitive information without proper consent or authorization.

### Regulatory

The EU AI Act is anticipated to be the first comprehensive AI law but will apply in 2025 at the earliest. The EU's General Data Protection Regulation (GDPR) does not specifically address AI but includes rules for data collection, data security, fairness and transparency, accuracy and reliability, and accountability, which can impact GenAI use. In the United States, AI regulation is included within broader consumer privacy laws. Ten US states have passed laws or have laws that will go into effect by the end of 2023.

Canada has so far only published a Voluntary Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems, however, the Artificial Intelligence and Data Act (AIDA) will have stronger requirements.

Federal organizations such as the US Equal Employment Opportunity Commission (EEOC), the Consumer Financial Protection Bureau (CFPB), the Federal Trade Commission (FTC), and the US Department of Justice's Civil Rights Division (DOJ) are closely monitoring hiring fairness.

Determine Country, State, or other Government specific AI compliance requirements.
Determine compliance requirements for restricting electronic monitoring of employees and employment-related automated decision systems (Vermont, California, Maryland, New York, New Jersey)
Determine compliance requirements for consent for facial recognition and the AI video analysis required (Illinois, Maryland, Washington, Vermont)
Review any AI tools in use or being considered for employee hiring or management.
Confirm the vendor’s compliance with applicable AI laws and best practices.
Ask and document any products using AI during the hiring process. Ask how the model was trained, and how it is monitored, and track any corrections made to avoid discrimination and bias.
Ask and document what accommodation options are included.
Ask and document whether the vendor collects confidential data.
Ask how the vendor or tool stores and deletes data and regulates the use of facial recognition and video analysis tools during pre-employment.
Review other organization-specific regulatory requirements with AI that may raise compliance issues. The Employee Retirement Income Security Act of 1974, for instance, has fiduciary duty requirements for retirement plans that a chatbot might not be able to meet. 

### Using or Implementing Large Language Model Solutions

Threat Model LLM components and architecture trust boundaries.
Data Security, verify how data is classified and protected based on sensitivity, including personal and proprietary business data. (How are user permissions managed, and what safeguards are in place?)
Access Control, implement least privilege access controls and implement defense-in-depth measures
Training Pipeline Security, require rigorous control around training data governance, pipelines, models, and algorithms.
Input and Output Security, evaluate input validation methods, as well as how outputs are filtered, sanitized, and approved.
Monitoring and Response, map workflows, monitoring, and responses to understand automation, logging, and auditing. Confirm audit records are secure.
Include application testing, source code review, vulnerability assessments, and red teaming in the production release process.
Check for existing vulnerabilities in the LLM model or supply chain.
Look into the effects of threats and attacks on LLM solutions, such as prompt injection, the release of sensitive information, and process manipulation.
Investigate the impact of attacks and threats to LLM models, including model poisoning, improper data handling, supply chain attacks, and model theft.
Supply Chain Security, request third-party audits, penetration testing, and code reviews for third-party providers. (both initially and on an ongoing basis)
Infrastructure Security, ask how often a vendor performs resilience testing? What are their SLAs in terms of availability, scalability, and performance?
Update incident response playbooks and include an LLM incident in tabletop exercises. 
Identify or expand metrics to benchmark generative cybersecurity AI against other approaches to measure expected productivity improvements.

### Testing, Evaluation, Verification, and Validation (TEVV)

NIST AI Framework recommends a continuous TEVV process throughout the AI lifecycle which includes the AI system operators, domain experts, AI designers, users, product developers, evaluators, and auditors. TEVV includes a range of tasks such as system validation, integration, testing, recalibration, and ongoing monitoring for periodic updates to navigate the risks and changes of the AI system.

Establish continuous testing, evaluation, verification, and validation throughout the AI model lifecycle.
Provide regular executive metrics and updates on AI Model functionality, security, reliability, and robustness.

### Model Cards and Risk Cards

Model cards and risk cards are foundational elements for increasing the transparency, accountability, and ethical deployment of Large Language Models (LLMs). Model cards help users understand and trust AI systems by providing standardized documentation on their design, capabilities, and constraints, leading them to make educated and safe applications. Risk cards supplement this by openly addressing potential negative consequences, such as biases, privacy problems, and security vulnerabilities, which encourages a proactive approach to harm prevention. These documents are critical for developers, users, regulators, and ethicists equally since they establish a collaborative atmosphere in which AI's social implications are carefully addressed and handled. These cards, developed and maintained by the organizations that created the models, play an important role in ensuring that AI technologies fulfill ethical standards and legal requirements, allowing for responsible research and deployment in the AI ecosystem.

Model cards include key attributes associated with the ML model:

Model details: Basic information about the model, i.e., name, version, and type (neural network, decision tree, etc.), and the intended use case.
Model architecture: Includes a description of the structure of the model, such as the number and type of layers, activation functions, and other key architectural choices.
Training data and methodology: Information about the data used to train the model, such as the size of the dataset, the data sources, and any preprocessing or data augmentation techniques used. It also includes details about the training methodology, such as the optimizer used, the loss function, and any hyperparameters that were tuned.
Performance metrics: Information about the model's performance on various metrics, such as accuracy, precision, recall, and F1 score. It may also include information about how the model performs on different subsets of the data.
Potential biases and limitations: Lists potential biases or limitations of the model, such as imbalanced training data, overfitting, or biases in the model's predictions. It may also include information about the model's limitations, such as its ability to generalize to new data or its suitability for certain use cases.
Responsible AI considerations: Any ethical or responsible AI considerations related to the model, such as privacy concerns, fairness, and transparency, or potential societal impacts of the model's use. It may also include recommendations for further testing, validation, or monitoring of the model.

The precise features contained in a model card may differ based on the model's context and intended usage, but the purpose is to give openness and accountability in the creation and deployment of machine learning models.

Review a models model card 
Review risk card if available
Establish a process to track and maintain model cards for any deployed model including models used through a third party.

### RAG: Large Language Model Optimization

Fine tuning, the traditional method for optimizing a pre-trained model, involved retraining an existing model on new, and domain-specific data, modifying it for performance on a task or application. Fine-tuning is expensive but essential to improve performance. 

Retrieval-Augmented Generation (RAG) has evolved as a more effective way of optimizing and augmenting the capabilities of large language models by retrieving pertinent data from up to date available knowledge sources. RAG can be customized for specific domains, optimizing the retrieval of domain-specific information and tailoring the generation process to the nuances of specialized fields. RAG is seen as a more efficient and transparent method for LLM optimization, particularly for problems where labeled data is limited or expensive to collect. One of the primary advantages of RAG is its support for continuous learning since new information can be continually updated at the retrieval stage.

The RAG implementation involves several key steps starting from embedding model deployment, indexing the knowledge library, to retrieving the most relevant documents for query processing. Efficient retrieval of the relevant context is made based on vector databases which are used for storage and querying of document embeddings.

#### RAG Reference
Retrieval Augmented Generation (RAG) & LLM: Examples
12 RAG Pain Points and Proposed Solutions

### AI Red Teaming

AI Red Teaming is an adversarial attack test simulation of the AI System to validate there aren’t any existing vulnerabilities which can be exploited by an attacker. It is a recommended practice by many regulatory and AI governing bodies including the Biden administration. Red-teaming alone is not a comprehensive solution to validate all real-world harms associated with AI systems and should be included with other forms of testing, evaluation, verification, and validation such as algorithmic impact assessments and external audits.

Incorporate Red Team testing as a standard practice for AI Models and applications.
## Resources

OWASP Top 10 for Large Language Models

![OWASP Top10 for LLM](images/GOV1_Fig_4_1.jpg)
##### Figure 4.1  OWASP Top 10 for Large Language Model Applications

![OWASP Top10 for LLM Visual](images/GOV1_Fig_4_2_en-ZZ.png)
##### Figure 4.2  OWASP Top 10 for Large Language Model Applications Visualized


### OWASP Resources

Using LLM solutions expands an organization's attack surface and presents new challenges, requiring special tactics and defenses. It also poses problems that are similar to known issues, and where there are already established cybersecurity procedures and mitigations. Integrating LLM cybersecurity with an organization's established cybersecurity controls, processes, and procedures allows an organization to reduce its vulnerability to threats.
How they integrate is available at the OWASP Integration Standards.

#### OWASP Resource
OWASP SAMM
#### Description
Software Assurance Maturity Model
#### Why it is Recommended & Where To Use It
Provides an effective and measurable way to analyze and improve an organization's secure development lifecycle. SAMM supports the complete software lifecycle. It is iterative and risk-driven, enabling organizations to identify and prioritize gaps in secure software development so resources for improving the process can be dedicated where efforts have the greatest improvement impact.

#### OWASP Resource
OWASP AI Exchange
OWASP Machine Learning Security Top 10
#### Description
OWASP Project to connect worldwide for an exchange on AI security, fostering standards alignment, and driving collaboration.
OWASP AI Exchange is the intake method for the OWASP AI Security and Privacy Guide
OWASP Machine Learning Security Top 10 security issues of machine learning systems
#### Why it is Recommended & Where To Use It
This project includes the ML Top 10 and is a live working document that provides clear and actionable insights on designing, creating, testing, and procuring secure and privacy-preserving AI systems. It is the best OWASP resource for AI global regulatory and privacy information.

#### OWASP Resource
Open Common Requirement Enumeration : OpenCRE
#### Description
OpenCRE is the interactive content-linking platform for uniting security standards and guidelines into one overview.
#### Why it is Recommended & Where To Use It
Use this site to search for standards. You can search by standard name or by control type.

#### OWASP Resource
OWASP Threat Modeling
#### Description
A structured, formal process for threat modeling of an application
#### Why it is Recommended & Where To Use It
Learn everything about Threat Modeling which is a structured representation of all the information that affects the security of an application.

#### OWASP Resource
OWASP CycloneDX
#### Description
OWASP CycloneDX is a full-stack Bill of Materials (BOM) standard that provides advanced supply chain capabilities for cyber risk reduction.
#### Why it is Recommended & Where To Use It
Modern software is assembled using third-party and open source components. They are glued together in complex and unique ways and integrated with original code to achieve the desired functionality. An SBOM provides an accurate inventory of all components which enables organizations to identify risk, allows for greater transparency, and enables rapid impact analysis.
EO 14028 provided minimum requirements for SBOM for federal systems.

#### OWASP Resource
OWASP Software Component Verification Standard (SCVS)
#### Description
A community-driven effort to establish a framework for identifying activities, controls, and best practices to identify and reduce risk in a software supply chain.
#### Why it is Recommended & Where To Use It
Use SCVS to develop a common set of activities, controls, and best practices that can reduce risk in a software supply chain and identify a baseline and path to mature software supply chain vigilance.

#### OWASP Resource
OWASP API Security Project
#### Description
API Security focuses on strategies and solutions to understand and mitigate the unique vulnerabilities and security risks of Application Programming Interfaces (APIs).
#### Why it is Recommended & Where To Use It
APIs are a foundational element of connecting applications and mitigating misconfigurations or vulnerabilities is mandatory to protect users and organizations. Use for security testing and red teaming the build and production environments.

#### OWASP Resource
OWASP Top 10 CI/CD Security Risks
#### Description
Helps defenders identify focus areas for securing their CI/CD ecosystem.
#### Why it is Recommended & Where To Use It
CI/CD environments, processes, and systems are the ecosystem of modern software organizations. They deliver code from an engineer's workstation to production. They have their unique attack surface and a frequent attack target. Use for security testing and red teaming the build and production environments. 

#### OWASP Resource
OWASP Application Security Verification Standard ASVS
#### Description
Application Security Verification Standard (ASVS) Project provides a basis for testing web application technical security controls and also provides developers with a list of requirements for secure development.
#### Why it is Recommended & Where To Use It
Cookbook for web application security requirements, security testing, and metrics. Use to establish security user stories and security use case release testing.

#### OWASP Resource
OWASP Threat and Safeguard Matrix (TaSM)
#### Description
An action oriented view to safeguard and enable the business
#### Why it is Recommended & Where To Use It
This matrix allows a company to overlay its major threats with the NIST Cyber Security Framework Functions (Identify, Protect, Detect, Respond, & Recover) to build a robust security plan. Use it as a dashboard to track and report on security across the organization.

#### OWASP Resource
Defect Dojo
#### Description
An open-source vulnerability management tool that streamlines the testing process by offering templating, report generation, metrics, and baseline self-service tools.
#### Why it is Recommended & Where To Use It
Use Defect Dojo to reduce the time for logging vulnerabilities with templates for vulnerabilities, imports for common vulnerability scanners, report generation, and metrics.


### MITRE Resources

The increased frequency of LLM threats emphasizes the value of a resilience-first approach to defending an organization's attack surface. Existing TTPS is combined with new attack surfaces and capabilities in LLM Adversary threats and mitigations. MITRE maintains a well-established and widely accepted mechanism for coordinating opponent tactics and procedures based on real-world observations.

Coordination and mapping of an organization's LLM Security Strategy to MITRE ATT&CK and MITRE ATLAS allows an organization to determine where LLM Security is covered by current processes such as API Security Standards or where security holes exist.

MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a framework, collection of data matrices, and assessment tool that was made by the MITRE Corporation to help organizations figure out how well their cybersecurity works across their entire digital attack surface and find holes that had not been found before. It is a knowledge repository that is used all over the world. The MITRE ATT&CK matrix contains a collection of strategies used by adversaries to achieve a certain goal. In the ATT&CK Matrix, these objectives are classified as tactics. The objectives are outlined in attack order, beginning with reconnaissance and progressing to the eventual goal of exfiltration or impact.

MITRE ATLAS, which stands for "Adversarial Threat Landscape for Artificial Intelligence Systems," is a knowledge base that is based on real-life examples of attacks on machine learning (ML) systems by bad actors. ATLAS is based on the MITRE ATT&CK architecture, and its tactics and procedures complement those found in ATT&CK.

#### MITRE Resource
MITRE ATT&CK
#### Description
Knowledge base of adversary tactics and techniques based on real-world observations
#### Why it is Recommended & Where To Use It
The ATT&CK knowledge base is used as a foundation for the development of specific threat models and methodologies. Map existing controls within the organization to adversary tactics and techniques to identify gaps or areas to test.

#### MITRE Resource
MITRE AT&CK Workbench
#### Description
Create or extend ATT&CK data in a local knowledge base
#### Why it is Recommended & Where To Use It
Host and manage a customized copy of the ATT&CK knowledge base. This local copy of the ATT&CK knowledge base can be extended with new or updated techniques, tactics, mitigation groups, and software that is specific to your organization.

#### MITRE Resource
MITRE ATLAS
#### Description
MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) is a knowledge base of adversary tactics, techniques, and case studies for machine learning (ML) systems based on real-world observations, demonstrations from ML red teams and security groups, and the state of the possible from academic research
#### Why it is Recommended & Where To Use It
Use it to map known ML vulnerabilities and map checks and controls for proposed projects or existing systems.

#### MITRE Resource
MITRE ATT&CK Powered Suit
#### Description
ATT&CK Powered Suit is a browser extension that puts the MITRE ATT&CK knowledge base at your fingertips.
#### Why it is Recommended & Where To Use It
Add to your browser to quickly search for tactics, techniques, and more without disrupting your workflow.

#### MITRE Resource
The Threat Report ATT&CK Mapper (TRAM)
#### Description
Automates TTP Identification in CTI Reports
#### Why it is Recommended & Where To Use It
Mapping TTPs found in CTI reports to MITRE ATT&CK is difficult, error prone, and time-consuming. TRAM uses LLMs to automate this process for the 50 most common techniques. Supports Juypter notebooks.

#### MITRE Resource
Attack Flow v2.1.0
#### Description
Attack Flow is a language for describing how cyber adversaries combine and sequence various offensive techniques to achieve their goals.
#### Why it is Recommended & Where To Use It
Attack Flow helps visualize how an attacker uses a technique, so defenders and leaders understand how adversaries operate and improve their defensive posture.

#### MITRE Resource
MITRE Caldera
#### Description
A cyber security platform (framework) designed to easily automate adversary emulation, assist manual red teams, and automate incident response.
#### Why it is Recommended & Where To Use It
Plugins are available for Caldera that help to expand the core capabilities of the framework and provide additional functionality, including agents, reporting, collections of TTPs, and others.
Here is the plugin library.

#### MITRE Resource
CALDERA plugin: Arsenal
#### Description
A plugin developed for adversary emulation of AI-enabled systems.
#### Why it is Recommended & Where To Use It
This plugin provides TTPs defined in MITRE ATLAS to interface with CALDERA.

#### MITRE Resource
Atomic Red Team
#### Description
Library of tests mapped to the MITRE ATT&CK framework.
#### Why it is Recommended & Where To Use It
Use to validate and test controls in an environment. Security teams can use Atomic Red Team to test controls.
You can execute atomic tests directly from the command line; no installation is required.

#### MITRE Resource
MITRE CTI Blueprints
#### Description
Automates Cyber Threat Intelligence reporting.
#### Why it is Recommended & Where To Use It
CTI Blueprints helps Cyber Threat Intelligence (CTI) analysts create high-quality, actionable reports more consistently and efficiently



### AI Vulnerability Repositories

AI Incident Database
A repository of articles about different times AI has failed in real-world applications and is maintained by a college research group and crowds sourced.
OECD AI Incidents Monitor (AIM)
Offers an accessible starting point for comprehending the landscape of AI-related challenges.


### Leading Companies Tracking AI Model Vulnerabilities

Huntr Bug Bounty : ProtectAI
Bug bounty platform for AI/ML
AI Vulnerability Database (AVID) : Garak
Database of model vulnerabilities
AI Risk Database: Robust Intelligence
Database of model vulnerabilities
LVE Repository
Open LLM Vulnerability and Exposures Repository


### AI Procurement Guidance

World Economic Forum: Adopting AI Responsibly: Guidelines for Procurement of AI Solutions by the Private Sector: Insight Report June 2023
The standard benchmarks and assessment criteria for procuring Artificial systems are in early development. This procurement guidelines provide organizations with a baseline of considerations for the end-to-end procurement process.

Use this guidance to augment an organization's existing Third Party Risk Supplier and Vendor procurement process. 

## Team

Thank you to the OWASP Top 10 for LLM Applications Cybersecurity and Governance Checklist Contributors.

### Checklist Contributors

Sandy Dunn
Heather Linn
John Sotiropoulos
Steve Wilson
Fabrizio Cilli
Aubrey King
Bob Simonoff
David Rowe
Ken Huang
Guilherme Junior
Andrea Succi
Jason Ross
Talesh Seeparsan
Anthony Glynn
Julie Tao
Cédric Lallier
Tetsuo Seto
Ads Dawson
